<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://bineshkumar.me/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site/feed.xml" rel="self" type="application/atom+xml"/><link href="https://bineshkumar.me/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-07T17:21:49+00:00</updated><id>https://bineshkumar.me/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site/feed.xml</id><title type="html">Binesh K Sadanandan</title><subtitle>Binesh Kumar, AI in Healthcare, Precision Oncology, Reinforcement Learning, Data Science. </subtitle><entry><title type="html">Multimodal Large Language Models in Healthcare: Current Applications and Validation Approaches</title><link href="https://bineshkumar.me/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site/blog/2025/multimodal-llms-healthcare-applications/" rel="alternate" type="text/html" title="Multimodal Large Language Models in Healthcare: Current Applications and Validation Approaches"/><published>2025-06-28T18:00:00+00:00</published><updated>2025-06-28T18:00:00+00:00</updated><id>https://bineshkumar.me/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site/blog/2025/multimodal-llms-healthcare-applications</id><content type="html" xml:base="https://bineshkumar.me/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site/blog/2025/multimodal-llms-healthcare-applications/"><![CDATA[<p>Multimodal Large Language Models (LLMs) can read images, text, time series, and audio all at once. They can spot findings in scans, summarize clinical notes, and track patient data over time. This review:</p> <ol> <li>Explains why multimodal models matter</li> <li>Describes leading models in medical imaging</li> <li>Covers models for electronic health records</li> <li>Lists key clinical validation datasets</li> <li>Notes deployment challenges</li> <li>Suggests future directions</li> </ol> <hr/> <h2 id="1-why-multimodal-ai-matters-in-healthcare">1. Why Multimodal AI Matters in Healthcare</h2> <p>Healthcare data comes in many forms</p> <ul> <li>Scans and slides: X rays, MRIs, CT scans, histology images</li> <li>Clinical notes: admission and discharge summaries, progress notes</li> <li>Time series: vital signs, lab trends, ECG waveforms</li> <li>Audio: heart and lung sounds, patient interviews</li> </ul> <p>A single model that handles all these inputs can</p> <ul> <li>Detect subtle abnormalities in images</li> <li>Summarize long reports in plain language</li> <li>Integrate chart trends with clinical context</li> <li>Support diagnosis and treatment planning</li> </ul> <hr/> <h2 id="2-vision-language-models-for-medical-imaging">2. Vision-Language Models for Medical Imaging</h2> <p>Multimodal models let us ask questions about images and get text answers. Here are four examples.</p> <h3 id="21-llava-med-15">2.1 LLaVA-Med 1.5</h3> <ul> <li><strong>Vision encoder</strong>: CLIP ViT-L/336px</li> <li><strong>Connector</strong>: MLP to map image features into text space</li> <li><strong>Language model</strong>: Vicuna v1.5 (13 B)</li> <li><strong>Data</strong>: 200 K image–text pairs from PubMed Central, GPT-4 synthetic instructions</li> <li><strong>Tasks</strong>: radiology VQA, visual report generation, pathology Q&amp;A</li> <li><strong>Performance</strong>: matches or beats benchmarks on VQA-Radiology and pathology tests</li> </ul> <h3 id="22-visual-med-alpaca">2.2 Visual Med-Alpaca</h3> <ul> <li><strong>Base</strong>: LLaMA-7 B with LoRA adapters</li> <li><strong>Pipeline</strong>: type classifier routes input, Med-GIT and DePlot experts process images, LLaMA core generates text</li> <li><strong>Data</strong>: 54 K Q&amp;A pairs from BigBIO and ROCO radiology sets, GPT-3.5–generated and human-filtered prompts</li> <li><strong>Notes</strong>: research use only, not FDA approved</li> <li><strong>Results</strong>: state-of-the-art accuracy on image QA benchmarks</li> </ul> <h3 id="23-chexagent">2.3 CheXagent</h3> <ul> <li><strong>Image encoder</strong>: SigLIP-Large (24 layers, 512 px)</li> <li><strong>Projector</strong>: MLP mapping 1 024 → 2 560 dims</li> <li><strong>Decoder</strong>: Phi-2.7 B trained on medical and scientific text</li> <li><strong>Training</strong>: 1 M+ chest X ray–report pairs, 2.7 B tokens from clinical notes and articles</li> <li><strong>Applications</strong>: draft radiology reports, detect abnormalities, explain findings</li> </ul> <h3 id="24-medgemma-4b-it">2.4 MedGemma-4B-IT</h3> <ul> <li><strong>Architecture</strong> <ul> <li>Decoder-only transformer (Gemma 3 base) with 4 B parameters</li> <li>SigLIP image encoder pre-trained on de-identified chest X rays, dermatology, ophthalmology, histopathology</li> </ul> </li> <li><strong>Inputs and outputs</strong> <ul> <li>Up to 128 K text tokens plus images (896 × 896 px, 256 tokens each)</li> <li>Generates text: reports, answers, summaries</li> </ul> </li> <li><strong>Technical specs</strong> <ul> <li>Context length: at least 128 K tokens</li> <li>Attention: grouped-query attention</li> <li>Release: July 9, 2025 (v1.0.1)</li> </ul> </li> <li> <p><strong>Key performance</strong></p> <table> <thead> <tr> <th>Task</th> <th>Base Gemma 3 4B</th> <th>MedGemma 4B-IT</th> </tr> </thead> <tbody> <tr> <td>MIMIC-CXR macro F1 (top 5)</td> <td>81.2</td> <td>88.9</td> </tr> <tr> <td>CheXpert macro F1 (top 5)</td> <td>32.6</td> <td>48.1</td> </tr> <tr> <td>CXR14 macro F1 (3 conds)</td> <td>32.0</td> <td>50.1</td> </tr> <tr> <td>SLAKE VQA token F1</td> <td>40.2</td> <td>72.3</td> </tr> <tr> <td>PathMCQA histopath accuracy</td> <td>37.1</td> <td>69.8</td> </tr> <tr> <td>EyePACS fundus accuracy</td> <td>14.4</td> <td>64.9</td> </tr> </tbody> </table> </li> <li><strong>Availability</strong> <ul> <li>Hosted on Hugging Face under Health AI Developer Foundations license</li> <li>Quick-start and fine-tuning notebooks on GitHub</li> </ul> </li> </ul> <hr/> <h2 id="3-language-models-for-electronic-health-records">3. Language Models for Electronic Health Records</h2> <p>Models can also read and reason over clinical text and signals.</p> <h3 id="31-gatortron">3.1 GatorTron</h3> <ul> <li><strong>Size</strong>: 110 M to 8.9 B parameters</li> <li><strong>Corpus</strong>: 82 B words of de-identified clinical text</li> <li><strong>Tasks</strong>: concept extraction, relation extraction, inference, question answering</li> <li><strong>Finding</strong>: larger models and more data boost all clinical NLP tasks</li> </ul> <h3 id="32-few-shot-health-learners">3.2 Few-Shot Health Learners</h3> <ul> <li><strong>Base</strong>: PaLM-24 B pretrained on 780 B tokens</li> <li><strong>Adaptation</strong>: few-shot fine-tuning on time series (ECG, vitals)</li> <li><strong>Uses</strong>: arrhythmia detection, activity recognition, calorie and stress estimation</li> <li><strong>Insight</strong>: large LLMs can ground numeric health data with minimal examples</li> </ul> <hr/> <h2 id="4-validation-datasets-for-clinical-ai">4. Validation Datasets for Clinical AI</h2> <p>Clinical deployment needs rigorous testing on real-world cases:</p> <ol> <li><strong>NEJM Clinicopathologic Cases</strong> <ul> <li>143 puzzles (2021–2024), scored by Bond Score (0–5) and Likert (0–2)</li> </ul> </li> <li><strong>NEJM Healer Series</strong> <ul> <li>20 cases, four stages: triage, exam, tests, management; R-IDEA rubric (0–10)</li> </ul> </li> <li><strong>Grey Matters Management</strong> <ul> <li>5 scenarios, 100-point rubric; compares GPT-4 vs physicians with and without AI</li> </ul> </li> <li><strong>MIMIC-IV-Ext Clinical Decision Making</strong> <ul> <li>2 400 ED visits for abdominal pain; diagnoses: appendicitis, cholecystitis, diverticulitis, pancreatitis</li> </ul> </li> <li><strong>Probabilistic Reasoning Challenges</strong> <ul> <li>Bayesian inference tasks with lab results; evaluate error in probability estimates</li> </ul> </li> </ol> <hr/> <h2 id="5-deployment-considerations">5. Deployment Considerations</h2> <p>Safe clinical use requires:</p> <ul> <li><strong>Privacy</strong> <ul> <li>De-identify data, encrypt records</li> </ul> </li> <li><strong>Generalization</strong> <ul> <li>Test on diverse hospitals and patient groups</li> </ul> </li> <li><strong>Explainability</strong> <ul> <li>Provide attention maps, saliency scores, counterfactuals</li> </ul> </li> <li><strong>Regulation</strong> <ul> <li>Define liability, follow FDA and CE guidelines</li> </ul> </li> </ul> <hr/> <h2 id="6-next-steps-and-future-directions">6. Next Steps and Future Directions</h2> <ol> <li><strong>Expand modalities</strong>: add genomics, proteomics, wearable data</li> <li><strong>Real-time AI</strong>: integrate into EHRs for live decision support</li> <li><strong>Personalization</strong>: fine-tune on individual patient histories</li> <li><strong>Unified benchmarks</strong>: cover performance, safety, fairness</li> </ol> <p>Multimodal LLMs can revolutionize healthcare, but careful validation and deployment are key to patient safety and trust.</p>]]></content><author><name>Binesh K Sadanandan</name></author><category term="research"/><category term="healthcare-ai"/><category term="multimodal-llms"/><category term="medical-imaging"/><category term="ehr-analysis"/><category term="clinical-ai"/><category term="validation-datasets"/><summary type="html"><![CDATA[Multimodal Large Language Models (LLMs) can read images, text, time series, and audio all at once. They can spot findings in scans, summarize clinical notes, and track patient data over time. This review:]]></summary></entry><entry><title type="html">The Evolution of Adversarial Robustness: From Neural Networks to Vision-Language Models</title><link href="https://bineshkumar.me/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site/blog/2025/adversarial-robustness-vision-language-models/" rel="alternate" type="text/html" title="The Evolution of Adversarial Robustness: From Neural Networks to Vision-Language Models"/><published>2025-05-22T17:00:00+00:00</published><updated>2025-05-22T17:00:00+00:00</updated><id>https://bineshkumar.me/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site/blog/2025/adversarial-robustness-vision-language-models</id><content type="html" xml:base="https://bineshkumar.me/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site/blog/2025/adversarial-robustness-vision-language-models/"><![CDATA[<p>Adversarial attacks expose hidden weaknesses in machine learning. This post traces how robustness research began with early neural networks and now extends to complex vision-language models. We highlight key discoveries, methods, and lessons for safe AI in healthcare.</p> <h2 id="1-early-insights-into-neural-networks-2014">1. Early Insights into Neural Networks (2014)</h2> <p><strong>Szegedy et al.</strong> first showed that deep nets have two surprising traits:</p> <ul> <li> <p><strong>Distributed knowledge</strong><br/> Every neuron carries bits of meaning, so no single node holds a concept alone.</p> </li> <li> <p><strong>Adversarial examples</strong><br/> Tiny, carefully chosen pixel changes can fool a classifier with high confidence.</p> </li> </ul> <p><strong>Main findings</strong>:</p> <ul> <li>Minimal perturbations (σ ≈ 0.06–0.14) yield 100% success against Inception models.</li> <li>Attacks transfer across models: 15–40% of examples that fool one network also fool others.</li> <li>The effect holds for linear models, convolutional nets, and unsupervised learners.</li> </ul> <p><strong>Why it happens</strong><br/> High-dimensional input spaces behave almost linearly in many directions. With limited training data, models leave blind spots that adversaries can exploit.</p> <p><strong>Impact</strong><br/> This work launched adversarial machine learning. A decade later, even advanced vision-language systems show the same core vulnerability.</p> <h2 id="2-robustness-in-vision-language-models-2023">2. Robustness in Vision-Language Models (2023)</h2> <p><strong>Zhao et al.</strong> studied how models like GPT-4 respond to image perturbations when accessed only via APIs.</p> <p><strong>Attack strategy</strong>:</p> <ol> <li><strong>Transfer stage</strong><br/> Generate initial adversarial images on a surrogate (CLIP or BLIP).</li> <li><strong>Query refinement</strong><br/> Fine-tune perturbations with a small number of black-box calls.</li> </ol> <p><strong>Key results</strong>:</p> <ul> <li>CLIP similarity scores jumped from 0.45 to over 0.80 using ε = 8/255 noise.</li> <li>Even small models (BLIP-Base, 224 M params) and large ones (MiniGPT-4, 14 B) were vulnerable.</li> <li>Only eight API queries were enough to reach high success rates.</li> </ul> <p><strong>Why it matters</strong><br/> Vision attacks are invisible to humans and fully automated. Standard text defenses cannot block them.</p> <h2 id="3-text-only-deception-of-vlms-2025">3. Text-Only Deception of VLMs (2025)</h2> <p><strong>Ahn et al.</strong> introduced the <strong>MAC (Multimodal Adversarial Compositionality)</strong> framework. They used LLMs to craft text prompts that mislead vision-language models without changing images.</p> <p><strong>Framework highlights</strong>:</p> <ul> <li><strong>Crossmodal similarity</strong><br/> Ensure generated text scores highly when paired with the original image.</li> <li><strong>Non-entailment</strong><br/> Text contradicts the true image content.</li> <li><strong>Lexical distance</strong><br/> Keep word changes subtle but effective.</li> <li><strong>Diversity</strong><br/> Produce varied attack patterns to avoid detection.</li> </ul> <p><strong>Performance</strong>:</p> <ul> <li>Success rates rose from 6.9% (zero-shot) to 42.1% with self-training.</li> <li>Transfer attacks fooled 23–41% of other models.</li> <li>Only four self-training steps matched a 16-step zero-shot baseline.</li> </ul> <p><strong>Takeaway</strong><br/> Even without touching pixels, clever text can exploit VLM weaknesses.</p> <h2 id="4-universal-surrogate-attacks-2025">4. Universal Surrogate Attacks (2025)</h2> <p><strong>Xu et al.</strong> showed that a single CLIP model can craft universal noise patterns that mislead many classifiers.</p> <p><strong>UnivIntruder method</strong>:</p> <ol> <li><strong>Surrogate alignment</strong><br/> Use CLIP text embeddings to guide perturbation direction.</li> <li><strong>Feature-space tuning</strong><br/> Target the embedding shifts that cause misclassification.</li> <li><strong>Robust transformations</strong><br/> Apply random crops and shifts to generalize perturbations.</li> </ol> <p><strong>Results</strong>:</p> <ul> <li>ImageNet top-1 attack rate: 85.1%</li> <li>CIFAR-10 attack rate: 99.4%</li> <li>Real-world services (Google, Baidu, GPT-4) saw 50–80% success.</li> <li>Required 80% fewer queries than previous black-box methods.</li> </ul> <p><strong>Defense notes</strong><br/> Adversarial training helps but cuts clean accuracy and demands heavy compute. Test-time filters also degrade performance.</p> <h2 id="5-pitfalls-in-medical-imaging-2023">5. Pitfalls in Medical Imaging (2023)</h2> <p><strong>Ghamizi et al.</strong> reevaluated adversarial claims on chest X-ray classifiers and found common flaws:</p> <ul> <li><strong>Binary labels only</strong><br/> Most studies ignore multi-label disease patterns.</li> <li><strong>Single dataset tests</strong><br/> Lack of cross-hospital validation hides generalization issues.</li> <li><strong>Unrealistic threat models</strong><br/> White-box attacks assume full model access.</li> <li><strong>Standard metrics fall short</strong><br/> Plain accuracy misses risks from co-occurring conditions.</li> </ul> <p><strong>Key insights</strong>:</p> <ul> <li>PadChest models were three times more robust than NIH-trained ones (53.6% vs 13.8%).</li> <li>Risk-based attacks dropped robust accuracy from 32.1% to 11.1%.</li> <li>Data variety mattered more than model architecture for true robustness.</li> </ul> <p><strong>Lesson</strong><br/> Healthcare demands domain-specific evaluations and realistic threat scenarios.</p> <h2 id="6-preserving-high-frequencies-in-vision-transformers-2024">6. Preserving High Frequencies in Vision Transformers (2024)</h2> <p><strong>Lin et al.</strong> identified that ViTs act like low-pass filters, losing fine details in diagrams and charts.</p> <p><strong>FM-ViT solution</strong>:</p> <ul> <li><strong>Fourier split</strong><br/> Separate image into low- and high-frequency components.</li> <li><strong>Adaptive reweighting</strong><br/> Learn weights for each frequency band per image.</li> <li><strong>Wavelet guidance</strong><br/> Use discrete wavelet transforms to set dynamic filters.</li> </ul> <p><strong>Impact</strong>:</p> <ul> <li>Significant accuracy gains on chart and diagram tasks.</li> <li>Better detection of small text and lines critical to medical scans.</li> </ul> <p><strong>Relevance</strong><br/> In X-rays, preserving edges can make the difference between spotting a nodule or missing it.</p> <h2 id="7-interpreting-vlm-attention-2024">7. Interpreting VLM Attention (2024)</h2> <p><strong>Stan et al.</strong> released <strong>LVLM-Interpret</strong>, a tool to visualize how vision-language models allocate attention.</p> <p><strong>Features</strong>:</p> <ul> <li><strong>Layer-by-layer maps</strong><br/> Show token interactions across image and text.</li> <li><strong>Relevancy scores</strong><br/> Use gradients to rank which patches drive each token.</li> <li><strong>Causal graphs</strong><br/> Identify minimal image regions needed for a given output.</li> </ul> <p><strong>Findings</strong>:</p> <ul> <li>Text prompts sometimes dominate over visual cues.</li> <li>Five to ten patches often suffice to fix the model’s answer.</li> <li>Bias patterns emerged when prompts contained sensitive topics.</li> </ul> <p><strong>Use case</strong><br/> Integrate this tool into clinical validation to catch unwanted text bias in medical VLMs.</p> <h2 id="8-token-activation-maps-2025">8. Token Activation Maps (2025)</h2> <p><strong>Yi Li et al.</strong> tackled the problem of early-token interference in multimodal generation.</p> <p><strong>TAM approach</strong>:</p> <ul> <li><strong>Causal inference</strong><br/> Estimate and remove influence from prior tokens.</li> <li><strong>Gaussian denoising</strong><br/> Filter out noise while keeping sharp details.</li> <li><strong>Fusion maps</strong><br/> Combine cleaned visual maps with textual relevance for each generated token.</li> </ul> <p><strong>Results</strong>:</p> <ul> <li>Up to 11-point F1‐IoU boost on COCO captions.</li> <li>Sharper object localization and clearer attribute highlighting.</li> </ul> <h2 id="conclusion-lessons-for-healthcare-ai">Conclusion: Lessons for Healthcare AI</h2> <ol> <li>Adversarial examples are a fundamental trait of high-dimensional models.</li> <li>Vision-language systems inherit and amplify these risks.</li> <li>Medical AI needs tailored threat models, multi-label tests, and cross-site validation.</li> <li>Frequency preservation and interpretability tools are vital for clinical trust.</li> <li>A unified evaluation across performance, safety, and fairness must guide deployment.</li> </ol> <p>By following these principles, we can build AI tools that serve patients safely and reliably.</p>]]></content><author><name>Binesh K Sadanandan</name></author><category term="research"/><category term="ai-safety"/><category term="adversarial-ml"/><category term="vision-language-models"/><category term="robustness"/><category term="medical-ai"/><category term="interpretability"/><summary type="html"><![CDATA[Adversarial attacks expose hidden weaknesses in machine learning. This post traces how robustness research began with early neural networks and now extends to complex vision-language models. We highlight key discoveries, methods, and lessons for safe AI in healthcare.]]></summary></entry><entry><title type="html">Large Language Models: From Architecture to Evaluation</title><link href="https://bineshkumar.me/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site/blog/2025/large-language-models-comprehensive-guide/" rel="alternate" type="text/html" title="Large Language Models: From Architecture to Evaluation"/><published>2025-04-18T16:00:00+00:00</published><updated>2025-04-18T16:00:00+00:00</updated><id>https://bineshkumar.me/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site/blog/2025/large-language-models-comprehensive-guide</id><content type="html" xml:base="https://bineshkumar.me/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site/blog/2025/large-language-models-comprehensive-guide/"><![CDATA[<p>Large Language Models (LLMs) have transformed how machines process text—and now they’re learning to see. In this guide, we:</p> <ol> <li>Show how we build chat-capable LLMs.</li> <li>Walk through instruction finetuning.</li> <li>Use Meta’s LLaMA as a concrete example of a multimodal LLM.</li> <li>Cover key evaluation metrics and best practices.</li> </ol> <hr/> <h2 id="1-from-text-to-conversation">1. From Text to Conversation</h2> <p>We start by teaching a model to predict words, then turn it into a helpful assistant.</p> <h3 id="11-pretraining-on-text">1.1 Pretraining on Text</h3> <ul> <li><strong>Data</strong>: Hundreds of billions of tokens from books, articles, code.</li> <li><strong>Task</strong>: Predict the next token in each sequence.</li> <li><strong>Model</strong>: A stack of transformer decoder blocks with self-attention and feed-forward layers.</li> <li><strong>Scale</strong>: <ul> <li>Embedding size (<code class="language-plaintext highlighter-rouge">d_model</code>), number of layers (<code class="language-plaintext highlighter-rouge">L</code>), and attention heads (<code class="language-plaintext highlighter-rouge">H</code>) set the model’s capacity.</li> <li>Doubling <code class="language-plaintext highlighter-rouge">d_model</code> roughly quadruples total parameters.</li> <li>Leading models range from 10 B to over 1 T parameters.</li> </ul> </li> <li><strong>Context window</strong>: <ul> <li>Early: 2 K tokens (e.g., GPT-3)</li> <li>Now: 100 K+ tokens for long documents</li> <li>Attention cost grows as $O(n^2)$, so long contexts need memory tricks.</li> </ul> </li> </ul> <blockquote> <p><strong>Tip:</strong> Use mixed precision (FP16 or BF16) to save GPU memory and speed up training.</p> </blockquote> <h3 id="12-instruction-finetuning">1.2 Instruction Finetuning</h3> <p>After pretraining, the model knows language patterns but not how to follow requests. We fix that by:</p> <ol> <li><strong>Collecting examples</strong> <ul> <li>Human-written prompts and ideal responses (translate, summarize, answer).</li> </ul> </li> <li><strong>Training</strong> <ul> <li>Continue cross-entropy on the response, conditioning on the instruction.</li> <li>Optionally apply RLHF (reinforcement learning from human feedback).</li> </ul> </li> <li><strong>Result</strong> <ul> <li>The model asks for clarification if unclear.</li> <li>It refuses harmful or off-limits requests.</li> <li>It shifts from “next-word guessing” to “helpful assistant.”</li> </ul> </li> </ol> <hr/> <h2 id="2-case-study-llama-as-a-multimodal-llm">2. Case Study: LLaMA as a Multimodal LLM</h2> <p>Meta’s LLaMA family illustrates how to add vision to a text model. LLaMA 3.2 processes images and text in one network.</p> <h3 id="21-why-multimodal">2.1 Why Multimodal?</h3> <ul> <li>Real-world tasks often mix text and images: recipes with photos, medical reports with scans, document Q&amp;A with figures.</li> <li>A single model that handles both can share knowledge across modes.</li> </ul> <h3 id="22-llama-32-architecture">2.2 LLaMA 3.2 Architecture</h3> <h4 id="221-vision-encoder">2.2.1 Vision Encoder</h4> <ol> <li><strong>Patch embedding</strong> <ul> <li>Split each image into a 32×32 grid of patches.</li> <li>Flatten each patch into a 1,280-dim vector.</li> </ul> </li> <li><strong>Two-stage encoder</strong> <ul> <li><strong>Local encoder</strong> (32 layers) captures textures and edges.</li> <li><strong>Global encoder</strong> (8 layers with gated attention) builds a wider context.</li> </ul> </li> <li><strong>Multi-scale features</strong> <ul> <li>Extract outputs from layers 3, 7, 15, 23, and 30 for richer signals.</li> </ul> </li> </ol> <h4 id="222-text-backbone">2.2.2 Text Backbone</h4> <ul> <li>Based on LLaMA 3.1</li> <li><strong>40 transformer layers</strong>, hidden size 4,096</li> <li>Alternates self-attention and cross-attention every 5 layers</li> </ul> <h4 id="223-cross-modal-bridge">2.2.3 Cross-Modal Bridge</h4> <ol> <li><strong>Projection layer</strong> <ul> <li>Map 7,680-dim visual features into the 4,096-dim text space.</li> </ul> </li> <li><strong>Cross-attention</strong> <ul> <li>At key layers, text tokens attend to image features.</li> </ul> </li> <li><strong>Gating</strong> <ul> <li>Control how much visual information flows in, preventing overload.</li> </ul> </li> </ol> <blockquote> <p><strong>Note:</strong> By sharing transformer blocks, LLaMA keeps parameter growth modest while gaining vision skills.</p> </blockquote> <hr/> <h2 id="3-evaluating-llms-a-holistic-view">3. Evaluating LLMs: A Holistic View</h2> <p>We measure models on multiple fronts—accuracy, fairness, cost, and robustness.</p> <h3 id="31-core-metrics">3.1 Core Metrics</h3> <table> <thead> <tr> <th>Metric</th> <th>What it measures</th> <th>When to use</th> </tr> </thead> <tbody> <tr> <td><strong>Perplexity</strong></td> <td>How well a model predicts text</td> <td>Language modeling</td> </tr> <tr> <td><strong>ROUGE</strong></td> <td>Overlap of generated vs. reference text</td> <td>Summarization, translation</td> </tr> </tbody> </table> <ul> <li> <p><strong>Perplexity</strong><br/> [ \text{Perplexity} = \exp\Bigl(-\tfrac{1}{D}\sum_{i=1}^D \log P(t_i \mid t_{&lt;i})\Bigr). ]<br/> Lower is better—fewer surprises.</p> </li> <li> <p><strong>ROUGE-1</strong><br/> [ \frac{\text{Matched words}}{\text{Words in reference}}. ]</p> </li> </ul> <h3 id="32-helm-framework">3.2 HELM Framework</h3> <p>A broad evaluation groups metrics into three pillars:</p> <ol> <li><strong>Efficiency</strong> <ul> <li>Training compute and energy use</li> <li>Inference latency and throughput</li> </ul> </li> <li><strong>Alignment</strong> <ul> <li>Fairness across demographics</li> <li>Bias and toxicity checks</li> </ul> </li> <li><strong>Capability</strong> <ul> <li>Task accuracy (F1, Exact Match, MRR)</li> <li>Calibration (ECE)</li> <li>Robustness to noise, paraphrases, typos</li> </ul> </li> </ol> <hr/> <h2 id="4-best-practices-and-takeaways">4. Best Practices and Takeaways</h2> <ol> <li><strong>Pretrain</strong> at large scale, using diverse text.</li> <li><strong>Finetune</strong> on clear, high-quality instruction data.</li> <li><strong>Integrate vision</strong> via cross-attention—LLaMA shows one path.</li> <li><strong>Evaluate broadly</strong>: don’t rely on a single number.</li> </ol> <p>By following this roadmap, you build LLMs that understand text, images, and user needs—while staying efficient, fair, and robust.</p>]]></content><author><name>Binesh K Sadanandan</name></author><category term="research"/><category term="deep-learning"/><category term="llms"/><category term="multimodal-ai"/><category term="evaluation-metrics"/><category term="llama"/><category term="transformers"/><summary type="html"><![CDATA[Large Language Models (LLMs) have transformed how machines process text—and now they’re learning to see. In this guide, we:]]></summary></entry><entry><title type="html">MLLMGuard: A Comprehensive Safety Framework for Multimodal Large Language Models</title><link href="https://bineshkumar.me/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site/blog/2025/mllmguard-safety-framework/" rel="alternate" type="text/html" title="MLLMGuard: A Comprehensive Safety Framework for Multimodal Large Language Models"/><published>2025-03-20T15:00:00+00:00</published><updated>2025-03-20T15:00:00+00:00</updated><id>https://bineshkumar.me/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site/blog/2025/mllmguard-safety-framework</id><content type="html" xml:base="https://bineshkumar.me/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site/blog/2025/mllmguard-safety-framework/"><![CDATA[<p>MLLMGuard evaluates safety in Multimodal Large Language Models across five critical dimensions. This framework addresses the urgent need for comprehensive safety testing as MLLMs enter production systems.</p> <h2 id="core-safety-dimensions">Core Safety Dimensions</h2> <h3 id="privacy">Privacy</h3> <p>Tests model behavior with sensitive information:</p> <ul> <li>Personal data exposure</li> <li>Corporate secrets</li> <li>Government-sensitive content</li> </ul> <p>Example: Extracting data from educational certificates</p> <h3 id="bias">Bias</h3> <p>Evaluates discriminatory responses:</p> <ul> <li>Stereotype reinforcement</li> <li>Prejudiced language</li> <li>Demographic discrimination</li> </ul> <p>Example: Identifying people by protected characteristics</p> <h3 id="toxicity">Toxicity</h3> <p>Measures harmful content handling:</p> <ul> <li>Hate speech detection</li> <li>Pornographic content</li> <li>Violence recognition</li> </ul> <p>Example: Processing hateful memes</p> <h3 id="truthfulness">Truthfulness</h3> <p>Identifies fabrication tendencies:</p> <ul> <li>Object hallucinations</li> <li>Adversarial robustness</li> <li>Consistency checks</li> </ul> <p>Example: Claiming non-existent objects in images</p> <h3 id="legality">Legality</h3> <p>Assesses legal boundary understanding:</p> <ul> <li>Restricted activities</li> <li>Safety violations</li> <li>Security threats</li> </ul> <p>Example: Responding to illegal activity queries</p> <h2 id="dataset-design">Dataset Design</h2> <p><strong>Scale</strong>: 2,282 image-text pairs<br/> <strong>Languages</strong>: 51.8% Chinese, 48.2% English<br/> <strong>Source</strong>: 82% from social media<br/> <strong>Quality</strong>: Expert-reviewed</p> <p><strong>Attack Techniques</strong>:</p> <ul> <li>Disguised prompts</li> <li>Noise injection</li> <li>Reverse Lubetion</li> <li>Harmful Scenario</li> </ul> <h2 id="guardrank-evaluator">GuardRank Evaluator</h2> <p>Automated scoring system using:</p> <ul> <li>LLaMA-2 + RoBERTa-Large</li> <li>78.5% accuracy (vs GPT-4’s 42.78%)</li> <li>Cost-effective alternative to human annotation</li> </ul> <h2 id="evaluation-results">Evaluation Results</h2> <p><strong>13 MLLMs tested</strong></p> <p>Commercial: GPT-4V, Gemini<br/> Open-source: LLaVA, MiniGPT-v2</p> <p><strong>Key Findings</strong>:</p> <ol> <li>Most models fail privacy, bias, and legality tests</li> <li>Only GPT-4V and MiniGPT-v2 show low attack success rates</li> <li>Hallucinations plague even advanced models</li> <li>Model size doesn’t predict safety performance</li> <li>Alignment quality beats parameter count</li> </ol> <h2 id="practical-impact">Practical Impact</h2> <p><strong>Applications</strong>:</p> <ul> <li>Content moderation</li> <li>Legal systems</li> <li>Healthcare</li> <li>Education</li> </ul> <p><strong>Cultural Requirements</strong>:</p> <ul> <li>Multilingual testing</li> <li>Diverse perspectives</li> <li>Cross-cultural safety</li> </ul> <h2 id="critical-balance">Critical Balance</h2> <p>Models must navigate:</p> <ul> <li>Truth vs harm prevention</li> <li>Helpfulness vs safety</li> <li>Transparency vs privacy</li> </ul> <h2 id="current-limitations">Current Limitations</h2> <ul> <li>Annotator demographic bias</li> <li>Manual creation costs</li> <li>Limited linguistic coverage</li> </ul> <h2 id="future-directions">Future Directions</h2> <ul> <li>Synthetic data generation</li> <li>Automated red teaming</li> <li>Dynamic benchmarks</li> <li>Expanded cultural representation</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>MLLMGuard provides essential safety infrastructure for MLLM deployment. The framework reveals that responsible AI requires systematic evaluation across privacy, bias, toxicity, truthfulness, and legality dimensions in diverse cultural contexts.</p> <p>Safety isn’t optional - it’s fundamental to beneficial AI deployment.</p> <h2 id="reference">Reference</h2> <p>Gu, T., Zhou, Z., Huang, K., Liang, D., Wang, Y., Zhao, H., Yao, Y., Qiao, X., Wang, K., Yang, Y., Teng, Y., Qiao, Y., &amp; Wang, Y. (2024). MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models. <em>arXiv preprint arXiv:2406.07594</em>. https://arxiv.org/pdf/2406.07594</p>]]></content><author><name>Binesh K Sadanandan</name></author><category term="research"/><category term="ai-safety"/><category term="multimodal-ai"/><category term="llm-safety"/><category term="vision-language-models"/><category term="ai-ethics"/><category term="red-teaming"/><summary type="html"><![CDATA[MLLMGuard evaluates safety in Multimodal Large Language Models across five critical dimensions. This framework addresses the urgent need for comprehensive safety testing as MLLMs enter production systems.]]></summary></entry><entry><title type="html">Understanding the Transformer Architecture: A Deep Dive</title><link href="https://bineshkumar.me/#%20the%20base%20hostname%20&%20protocol%20for%20your%20site/blog/2025/understanding-transformers-architecture/" rel="alternate" type="text/html" title="Understanding the Transformer Architecture: A Deep Dive"/><published>2025-02-15T14:00:00+00:00</published><updated>2025-02-15T14:00:00+00:00</updated><id>https://bineshkumar.me/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site/blog/2025/understanding-transformers-architecture</id><content type="html" xml:base="https://bineshkumar.me/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site/blog/2025/understanding-transformers-architecture/"><![CDATA[<p>The transformer architecture changed natural language processing when Vaswani et al. introduced it in “Attention is All You Need.” This post explains how transformers work, focusing on their key components and the attention mechanism.</p> <h2 id="what-is-a-transformer">What is a Transformer?</h2> <p>A transformer has two main parts:</p> <p><strong>Encoder</strong>: Reads input text and creates contextual vectors<br/> <strong>Decoder</strong>: Uses these vectors to generate output text</p> <p>Both parts use self-attention to understand how words relate to each other in a sequence.</p> <h2 id="why-use-transformers">Why Use Transformers?</h2> <p>Transformers process entire sequences at once, unlike RNNs and LSTMs that process one word at a time. This gives three benefits:</p> <p>• Removes sequential bottlenecks<br/> • Captures long-range dependencies better<br/> • Trains much faster</p> <h2 id="the-encoder">The Encoder</h2> <p>The encoder stacks 6 identical layers. Each layer contains:</p> <h3 id="1-multi-head-self-attention">1. Multi-Head Self-Attention</h3> <p>Every token looks at every other token in the sequence. This captures word relationships regardless of distance. The model adds positional encodings to embeddings so it knows word order.</p> <h3 id="2-add--norm">2. Add &amp; Norm</h3> <p><strong>Add</strong>: Adds the input back to the output (residual connection)<br/> <strong>Norm</strong>: Standardizes values across layers</p> <p>These prevent vanishing gradients and help the model learn.</p> <h3 id="3-feedforward-network">3. Feedforward Network</h3> <p>A simple neural network that refines token representations.</p> <h3 id="4-more-residual-connections">4. More Residual Connections</h3> <p>Each sublayer passes its input forward with its output, which stabilizes training.</p> <h2 id="the-decoder">The Decoder</h2> <p>The decoder also stacks identical layers, but each has three parts:</p> <h3 id="1-masked-self-attention">1. Masked Self-Attention</h3> <p>The decoder looks at its previous outputs but can’t see future tokens. This maintains causality during generation.</p> <h3 id="2-cross-attention">2. Cross-Attention</h3> <p>Here the decoder connects to the encoder:</p> <p>• Uses encoder output as keys and values<br/> • Uses its own state as queries<br/> • Focuses on relevant input parts for each output token</p> <h3 id="3-feedforward-layer">3. Feedforward Layer</h3> <p>Refines the decoder’s representations, just like in the encoder.</p> <h2 id="how-attention-works">How Attention Works</h2> <h3 id="core-components">Core Components</h3> <p><strong>Input Embeddings</strong>: Word vectors (example: [“we,” “train,” “a,” “transformer,” “model”])</p> <p><strong>Weight Matrices</strong>:<br/> • \(W_Q\): Creates query vectors<br/> • \(W_K\): Creates key vectors<br/> • \(W_V\): Creates value vectors</p> <h3 id="the-attention-process">The Attention Process</h3> <ol> <li> <p><strong>Create Q, K, V matrices</strong><br/> Multiply input X by weight matrices. Each token gets: • Query (q): What information it needs • Key (k): What information it offers • Value (v): Its actual content</p> </li> <li> <p><strong>Calculate attention scores</strong><br/> For each token, compute dot product of its query with all keys. This measures relevance.</p> </li> <li> <p><strong>Scale the scores</strong><br/> Divide by \(\sqrt{d_k}\) to prevent large values that break softmax.</p> </li> <li> <p><strong>Apply causal mask</strong><br/> Add mask to hide future tokens. For position 2:<br/> <code class="language-plaintext highlighter-rouge">mask = [0, 0, -∞, -∞]</code></p> </li> <li> <p><strong>Apply softmax</strong><br/> Convert scores to probabilities (attention weights).</p> </li> <li> <p><strong>Compute weighted sum</strong><br/> Multiply values by attention weights and sum:<br/> \(g_i = \sum_{j=1}^{n} w_j \cdot v_j\)</p> </li> </ol> <h3 id="the-complete-formula">The Complete Formula</h3> \[G = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + M\right) V\] <p>Where:<br/> • Q, K, V = Query, key, value matrices<br/> • M = Causal mask</p> <h2 id="byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</h2> <p>BPE is a tokenization method that breaks text into subwords. It balances vocabulary size with the ability to handle rare words.</p> <h3 id="why-bpe">Why BPE?</h3> <p>Traditional tokenization has problems:<br/> • Poor computational efficiency<br/> • Can’t handle unseen words</p> <p>BPE solves these by learning common subword patterns.</p> <h3 id="how-bpe-works">How BPE Works</h3> <ol> <li><strong>Start</strong>: Begin with individual characters plus end-of-word marker (_)</li> <li><strong>Tokenize</strong>: Break text into these basic tokens</li> <li><strong>Merge</strong>: Find the most common adjacent pair and merge them. This: • Shortens the sequence • Adds new token to vocabulary</li> <li><strong>Stop</strong>: After set number of merges or when no common pairs remain</li> </ol> <h2 id="summary">Summary</h2> <p>Transformers excel because they:<br/> • Process sequences in parallel<br/> • Capture long-range dependencies<br/> • Use attention to understand word relationships</p> <p>This architecture powers BERT, GPT, and other breakthrough models in AI.</p> <h2 id="reference">Reference</h2> <p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. <em>Advances in neural information processing systems</em>, 30.</p>]]></content><author><name>Binesh K Sadanandan</name></author><category term="research"/><category term="deep-learning"/><category term="transformers"/><category term="attention-mechanism"/><category term="neural-networks"/><category term="nlp"/><summary type="html"><![CDATA[The transformer architecture changed natural language processing when Vaswani et al. introduced it in “Attention is All You Need.” This post explains how transformers work, focusing on their key components and the attention mechanism.]]></summary></entry></feed>