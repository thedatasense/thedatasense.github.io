<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Understanding the Transformer Architecture: A Deep Dive | Binesh K Sadanandan </title> <meta name="author" content="Binesh K Sadanandan"> <meta name="description" content="Binesh Kumar, AI in Healthcare, Precision Oncology, Reinforcement Learning, Data Science. "> <meta name="keywords" content="binesh kumar, binesh, sadanandan, sail, university of new haven, medtronic"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bineshkumar.me/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site/blog/2025/understanding-transformers-architecture/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Binesh K Sadanandan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Understanding the Transformer Architecture: A Deep Dive</h1> <p class="post-meta"> February 15, 2025 • Binesh K Sadanandan </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/transformers"> <i class="fa-solid fa-hashtag fa-sm"></i> transformers</a>   <a href="/blog/tag/attention-mechanism"> <i class="fa-solid fa-hashtag fa-sm"></i> attention-mechanism</a>   <a href="/blog/tag/neural-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> neural-networks</a>   <a href="/blog/tag/nlp"> <i class="fa-solid fa-hashtag fa-sm"></i> nlp</a>     ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   <a href="/blog/category/deep-learning"> <i class="fa-solid fa-tag fa-sm"></i> deep-learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>The transformer architecture changed natural language processing when Vaswani et al. introduced it in “Attention is All You Need.” This post explains how transformers work, focusing on their key components and the attention mechanism.</p> <h2 id="what-is-a-transformer">What is a Transformer?</h2> <p>A transformer has two main parts:</p> <p><strong>Encoder</strong>: Reads input text and creates contextual vectors<br> <strong>Decoder</strong>: Uses these vectors to generate output text</p> <p>Both parts use self-attention to understand how words relate to each other in a sequence.</p> <h2 id="why-use-transformers">Why Use Transformers?</h2> <p>Transformers process entire sequences at once, unlike RNNs and LSTMs that process one word at a time. This gives three benefits:</p> <p>• Removes sequential bottlenecks<br> • Captures long-range dependencies better<br> • Trains much faster</p> <h2 id="the-encoder">The Encoder</h2> <p>The encoder stacks 6 identical layers. Each layer contains:</p> <h3 id="1-multi-head-self-attention">1. Multi-Head Self-Attention</h3> <p>Every token looks at every other token in the sequence. This captures word relationships regardless of distance. The model adds positional encodings to embeddings so it knows word order.</p> <h3 id="2-add--norm">2. Add &amp; Norm</h3> <p><strong>Add</strong>: Adds the input back to the output (residual connection)<br> <strong>Norm</strong>: Standardizes values across layers</p> <p>These prevent vanishing gradients and help the model learn.</p> <h3 id="3-feedforward-network">3. Feedforward Network</h3> <p>A simple neural network that refines token representations.</p> <h3 id="4-more-residual-connections">4. More Residual Connections</h3> <p>Each sublayer passes its input forward with its output, which stabilizes training.</p> <h2 id="the-decoder">The Decoder</h2> <p>The decoder also stacks identical layers, but each has three parts:</p> <h3 id="1-masked-self-attention">1. Masked Self-Attention</h3> <p>The decoder looks at its previous outputs but can’t see future tokens. This maintains causality during generation.</p> <h3 id="2-cross-attention">2. Cross-Attention</h3> <p>Here the decoder connects to the encoder:</p> <p>• Uses encoder output as keys and values<br> • Uses its own state as queries<br> • Focuses on relevant input parts for each output token</p> <h3 id="3-feedforward-layer">3. Feedforward Layer</h3> <p>Refines the decoder’s representations, just like in the encoder.</p> <h2 id="how-attention-works">How Attention Works</h2> <h3 id="core-components">Core Components</h3> <p><strong>Input Embeddings</strong>: Word vectors (example: [“we,” “train,” “a,” “transformer,” “model”])</p> <p><strong>Weight Matrices</strong>:<br> • \(W_Q\): Creates query vectors<br> • \(W_K\): Creates key vectors<br> • \(W_V\): Creates value vectors</p> <h3 id="the-attention-process">The Attention Process</h3> <ol> <li> <p><strong>Create Q, K, V matrices</strong><br> Multiply input X by weight matrices. Each token gets: • Query (q): What information it needs • Key (k): What information it offers • Value (v): Its actual content</p> </li> <li> <p><strong>Calculate attention scores</strong><br> For each token, compute dot product of its query with all keys. This measures relevance.</p> </li> <li> <p><strong>Scale the scores</strong><br> Divide by \(\sqrt{d_k}\) to prevent large values that break softmax.</p> </li> <li> <p><strong>Apply causal mask</strong><br> Add mask to hide future tokens. For position 2:<br> <code class="language-plaintext highlighter-rouge">mask = [0, 0, -∞, -∞]</code></p> </li> <li> <p><strong>Apply softmax</strong><br> Convert scores to probabilities (attention weights).</p> </li> <li> <p><strong>Compute weighted sum</strong><br> Multiply values by attention weights and sum:<br> \(g_i = \sum_{j=1}^{n} w_j \cdot v_j\)</p> </li> </ol> <h3 id="the-complete-formula">The Complete Formula</h3> \[G = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + M\right) V\] <p>Where:<br> • Q, K, V = Query, key, value matrices<br> • M = Causal mask</p> <h2 id="byte-pair-encoding-bpe">Byte Pair Encoding (BPE)</h2> <p>BPE is a tokenization method that breaks text into subwords. It balances vocabulary size with the ability to handle rare words.</p> <h3 id="why-bpe">Why BPE?</h3> <p>Traditional tokenization has problems:<br> • Poor computational efficiency<br> • Can’t handle unseen words</p> <p>BPE solves these by learning common subword patterns.</p> <h3 id="how-bpe-works">How BPE Works</h3> <ol> <li> <strong>Start</strong>: Begin with individual characters plus end-of-word marker (_)</li> <li> <strong>Tokenize</strong>: Break text into these basic tokens</li> <li> <strong>Merge</strong>: Find the most common adjacent pair and merge them. This: • Shortens the sequence • Adds new token to vocabulary</li> <li> <strong>Stop</strong>: After set number of merges or when no common pairs remain</li> </ol> <h2 id="summary">Summary</h2> <p>Transformers excel because they:<br> • Process sequences in parallel<br> • Capture long-range dependencies<br> • Use attention to understand word relationships</p> <p>This architecture powers BERT, GPT, and other breakthrough models in AI.</p> <h2 id="reference">Reference</h2> <p>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … &amp; Polosukhin, I. (2017). Attention is all you need. <em>Advances in neural information processing systems</em>, 30.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/large-language-models-comprehensive-guide/">Large Language Models: From Architecture to Evaluation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/adversarial-robustness-vision-language-models/">The Evolution of Adversarial Robustness: From Neural Networks to Vision-Language Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/multimodal-llms-healthcare-applications/">Multimodal Large Language Models in Healthcare: Current Applications and Validation Approaches</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mllmguard-safety-framework/">MLLMGuard: A Comprehensive Safety Framework for Multimodal Large Language Models</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Binesh K Sadanandan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>