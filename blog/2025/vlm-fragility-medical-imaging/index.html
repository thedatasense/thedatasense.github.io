<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> When AI Radiologists Get Confused: The Critical Challenge of VLM Robustness in Medical Diagnostics | Binesh K Sadanandan </title> <meta name="author" content="Binesh K Sadanandan"> <meta name="description" content="Binesh Kumar, AI in Healthcare, Precision Oncology, Reinforcement Learning, Data Science. "> <meta name="keywords" content="binesh kumar, binesh, sadanandan, sail, university of new haven, medtronic"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bineshkumar.me/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site/blog/2025/vlm-fragility-medical-imaging/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Binesh K Sadanandan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research Notes </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">When AI Radiologists Get Confused: The Critical Challenge of VLM Robustness in Medical Diagnostics</h1> <p class="post-meta"> July 15, 2025 • Binesh K Sadanandan </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/vision-language-models"> <i class="fa-solid fa-hashtag fa-sm"></i> vision-language-models</a>   <a href="/blog/tag/medical-imaging"> <i class="fa-solid fa-hashtag fa-sm"></i> medical-imaging</a>   <a href="/blog/tag/ai-safety"> <i class="fa-solid fa-hashtag fa-sm"></i> ai-safety</a>   <a href="/blog/tag/robustness"> <i class="fa-solid fa-hashtag fa-sm"></i> robustness</a>   <a href="/blog/tag/chest-xray"> <i class="fa-solid fa-hashtag fa-sm"></i> chest-xray</a>     ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   <a href="/blog/category/medical-ai"> <i class="fa-solid fa-tag fa-sm"></i> medical-ai</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Picture this: You’re in the emergency room with chest pain and shortness of breath. The doctor orders a chest X-ray, and while waiting for the radiologist, you pull out your phone. Could ChatGPT help interpret what’s wrong? You’ve used it for math problems and recipe suggestions. Surely it could read an X-ray?</p> <p>This isn’t a hypothetical anymore. We’re already there. An Australian study found that 9.9% of adults had used ChatGPT for health questions in just six months, with nearly 40% of non-users considering it. When people get health advice from ChatGPT, nearly half simply follow it. No questions asked, no double-checking with their doctor.</p> <p>Our recent research reveals that when these sophisticated AI systems move from answering text questions to interpreting medical images, they become dangerously brittle. We evaluated 125 chest X-ray interpretations using state-of-the-art vision language models, including Google’s MedGemma 4B and GPT-4V. Simply changing “vascular dilation” to “vascular congestion” in a question made the same AI system provide completely different diagnoses for the same X-ray image.</p> <hr> <h2 id="the-promise-of-vlm-based-radiologists">The Promise of VLM-Based Radiologists</h2> <p>When we first started working with vision language models for medical imaging, the promise seemed clear. Unlike traditional AI that just spits out labels like “pneumonia: 87% probability,” these models could actually explain what they saw. They’d tell you why they thought something looked abnormal. You could ask follow-up questions. Feed them a patient’s history and watch them adjust their interpretation accordingly.</p> <p>But here’s what we discovered matters just as much as accuracy: consistency. We call it robustness in the lab, but what it really means is whether the AI gives you the same answer when you ask the same question slightly differently. Think about it. A radiologist doesn’t suddenly see pneumonia just because you say “chest radiograph” instead of “X-ray.” They know “lung volumes” and “lung capacity” mean the same thing.</p> <p>Yet that’s exactly what happens with today’s most advanced models. And we’re not talking about edge cases or trick questions. We tested basic medical synonyms, the kind any first-year resident would recognize as identical. The models fell apart.</p> <hr> <h2 id="when-terminology-becomes-a-diagnostic-trap">When Terminology Becomes a Diagnostic Trap</h2> <p>Let’s walk through real examples from our evaluation that show how catastrophically these models can fail.</p> <h3 id="case-1-the-vascular-confusion">Case 1: The Vascular Confusion</h3> <p>We showed a chest X-ray to one of the most advanced vision language models available. Asked about vascular findings. The model correctly identified pulmonary vascular dilation, which is exactly what we’d expect. It’s a widening of blood vessels that might indicate various conditions but isn’t immediately life threatening.</p> <p>Then we changed two words. Just two. “Vascular dilation” became “vascular congestion.”</p> <figure> <picture> <img src="/assets/case-1.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Case 1: Vascular Terminology Confusion" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Suddenly the model was talking about cardiac congestion. Possible heart failure. Recommending completely different follow-up procedures. Same image, nearly identical question, completely different medical pathway. The clinical implications hit us immediately. A patient might get rushed into unnecessary cardiac workup while their actual condition goes untreated. Or worse, someone might start urgent cardiac treatment for what’s actually a non-cardiac issue.</p> <h3 id="case-2-the-imaginary-pneumonia">Case 2: The Imaginary Pneumonia</h3> <p>This one still makes us shake our heads. We had an X-ray showing clear pleural effusion. That’s fluid around the lungs, often serious enough to need drainage. The model saw it correctly when we asked about lung findings.</p> <p>But when we added the phrase “chest radiograph” to our question? The model suddenly “saw” pneumonia that wasn’t there.</p> <figure> <picture> <img src="/assets/case-2.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Case 2: The Imaginary Pneumonia" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>It didn’t just add pneumonia to its diagnosis. It completely forgot about the pleural effusion and started recommending antibiotics. This isn’t just wrong. It’s actively harmful. A patient with fluid crushing their lungs needs drainage, not antibiotics for an infection that doesn’t exist.</p> <h3 id="case-3-the-vanishing-diagnosis">Case 3: The Vanishing Diagnosis</h3> <p>Perhaps most concerning was when changing “lung volumes” to “lung capacity” made critical findings disappear entirely. The model went from correctly identifying pleural effusion and potential cardiac issues to completely missing the effusion and focusing only on cardiac problems.</p> <figure> <picture> <img src="/assets/case-3.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Case 3: The Vanishing Diagnosis" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Pleural effusion can kill you if it’s not treated. It can lead to respiratory failure. Yet a simple synonym made the AI blind to its presence. The model confidently described other findings while missing the one thing that might send someone to the ICU.</p> <p>What makes these failures so unsettling is their unpredictability. You can’t train staff to avoid certain phrases or create a list of “safe” terminology. The brittleness runs deeper than that.</p> <hr> <h2 id="making-sense-of-the-brittleness-what-were-learning-in-the-lab">Making Sense of the Brittleness: What We’re Learning in the Lab</h2> <p>The brittleness we observed in chest X-ray VLMs sent us down a research rabbit hole. How could models that seem so sophisticated fail so spectacularly when we barely changed our words? We needed a systematic way to measure this vulnerability, which led us to develop VSF-Med (Vulnerability Scoring Framework for Medical Vision-Language Models) here at the SAIL Lab at University of New Haven.</p> <h3 id="vsf-med-our-systematic-approach">VSF-Med: Our Systematic Approach</h3> <p>VSF-Med isn’t just another benchmark. It’s our attempt to quantify exactly how and why these models break. We evaluated 68,478 attack scenarios across five models including:</p> <ul> <li> <strong>CheXagent 8B</strong>: Medical-specialized model</li> <li> <strong>Llama 3.2 11B Vision</strong>: General-purpose VLM</li> <li> <strong>GPT-4o</strong>: State-of-the-art multimodal model</li> <li> <strong>Google MedGemma 4B</strong>: Medical-focused model</li> <li> <strong>GPT-4V</strong>: Previous generation flagship</li> </ul> <p>The framework measures vulnerability across nine different attack vectors, giving us concrete numbers for what we’d been observing anecdotally.</p> <h3 id="the-sobering-results">The Sobering Results</h3> <p>What we found was concerning. Even CheXagent 8B, a model specifically trained for medical imaging, showed moderate vulnerability (z-score: 0.68) to our prompt injection attacks. That “dilation vs congestion” problem we showed you earlier? Not an isolated incident.</p> <p><strong>Key findings</strong>:</p> <ul> <li>Performance scores ranged from <strong>7 to 97</strong> across different phrasings of identical X-ray questions</li> <li>Medical-specialized models demonstrated only <strong>36% better resilience</strong> compared to general-purpose VLMs</li> <li>Current best-in-class models still have vulnerability spreads exceeding 0.3 standard deviations</li> </ul> <p>Better than general models, yes. Good enough for clinical use? Not even close.</p> <h3 id="why-are-these-models-so-fragile">Why Are These Models So Fragile?</h3> <p>We have theories we’re exploring:</p> <ol> <li> <p><strong>Contrastive Learning Issues</strong>: Many vision-language models use contrastive learning during training, where they learn to match images with text descriptions. This might create brittle associations between specific phrases and visual features.</p> </li> <li> <p><strong>The Alignment Problem</strong>: These models are fine-tuned to be helpful and responsive, which might make them overeager to provide different answers when prompted differently. They’re trying so hard to be useful that they forget to be consistent.</p> </li> <li> <p><strong>Medical Language Complexity</strong>: Radiological language is precise but full of synonyms. Models trained on general text might not grasp that “increased opacity” and “increased density” mean the same thing in a chest X-ray context.</p> </li> <li> <p><strong>Architectural Limitations</strong>: The transformer architecture itself might contribute to this sensitivity. Attention mechanisms that work beautifully for general language tasks might amplify small prompt variations in high-stakes medical contexts.</p> </li> </ol> <hr> <h2 id="what-this-means-for-medical-ai">What This Means for Medical AI</h2> <p>The momentum behind medical AI is undeniable. Just this week, Mayo Clinic Press highlighted how AI is already being used for stroke diagnosis, heart failure detection, and cancer screening. They describe an optimistic future where “AI has the potential to improve the work of human healthcare teams, making care more personal and effective.”</p> <p>While this enthusiasm is understandable given AI’s promise, our research suggests we need to address fundamental robustness issues before these systems can truly deliver on that potential.</p> <h3 id="safety-implications">Safety Implications</h3> <ol> <li> <strong>Diagnostic Inconsistency</strong>: Same image, different terminology = different diagnoses</li> <li> <strong>Clinical Risk</strong>: Unreliable AI could misguide medical decisions</li> <li> <strong>Trust Issues</strong>: Healthcare providers need consistent, predictable AI behavior</li> <li> <strong>Patient Safety</strong>: When people follow AI medical advice without verification, inconsistency becomes dangerous</li> </ol> <h3 id="the-real-world-context">The Real-World Context</h3> <p>Remember those statistics we opened with? People are already using these tools for health decisions. An Australian study found that people with limited health literacy use ChatGPT at nearly twice the rate of others (18.4% vs 9.4%). Those from non-English speaking backgrounds? Even higher at 29.2%. These are exactly the populations who might be most vulnerable to inconsistent AI responses.</p> <hr> <h2 id="future-directions">Future Directions</h2> <p>Our research highlights the urgent need for:</p> <ol> <li> <strong>Robust Training Methods</strong>: VLMs that maintain consistency across terminology variations</li> <li> <strong>Comprehensive Testing</strong>: Systematic evaluation of medical AI before clinical deployment</li> <li> <strong>Safety Frameworks</strong>: Guidelines for reliable medical AI implementation</li> <li> <strong>Architectural Innovations</strong>: New approaches that improve multimodal robustness</li> </ol> <h3 id="open-science-commitment">Open Science Commitment</h3> <p>Our VSF-Med framework is completely open source because we believe this problem is too important for any single team to tackle alone. We’ve made it so researchers anywhere can benchmark their medical VLM with a single command, generating over 30,000 adversarial test cases automatically.</p> <p>We’re diving deeper into architectural modifications that might improve robustness. We’re exploring whether different training objectives could create more stable image-text associations. And we’re working with clinicians to understand which types of brittleness pose the greatest real-world risks.</p> <hr> <h2 id="the-path-forward">The Path Forward</h2> <p>Medical AI has immense potential to revolutionize healthcare, from reducing diagnostic errors to making expertise available in underserved areas. But as our research shows, we’re not there yet. The brittleness we’ve uncovered isn’t just a technical curiosity—it’s a fundamental barrier to safe clinical deployment.</p> <p>Until we can get vulnerability spreads much, much lower, these systems remain too fragile for autonomous clinical use. This isn’t about being pessimistic about AI. It’s about being realistic about what needs to be fixed before we can responsibly deploy these powerful tools in life-and-death situations.</p> <p>Because ultimately, this isn’t just an interesting technical puzzle. It’s about making sure AI tools genuinely help rather than harm when lives are on the line.</p> <hr> <h2 id="publication-details">Publication Details</h2> <p><strong>Paper</strong>: VSF-Med: A Vulnerability Scoring Framework for Medical Vision-Language Models<br> <strong>Authors</strong>: Binesh K Sadanandan, Vahid Behzadan<br> <strong>Journal</strong>: arXiv preprint arXiv:2507.00052<br> <strong>Year</strong>: 2025<br> <strong>GitHub</strong>: <a href="https://github.com/sail-lab" rel="external nofollow noopener" target="_blank">Open-source framework available</a><br> <strong>Link</strong>: <a href="https://sail-lab.org/vlm-fragility-medical-imaging/" rel="external nofollow noopener" target="_blank">https://sail-lab.org/vlm-fragility-medical-imaging/</a></p> <p><em>Research conducted at the SAIL Lab, University of New Haven</em></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/adversarial-robustness-vision-language-models/">The Evolution of Adversarial Robustness: From Neural Networks to Vision-Language Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/multimodal-llms-healthcare-applications/">Multimodal Large Language Models in Healthcare: Current Applications and Validation Approaches</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mllmguard-safety-framework/">MLLMGuard: A Comprehensive Safety Framework for Multimodal Large Language Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/large-language-models-comprehensive-guide/">Large Language Models: From Architecture to Evaluation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/understanding-transformers-architecture/">Understanding the Transformer Architecture: A Deep Dive</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Binesh K Sadanandan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>