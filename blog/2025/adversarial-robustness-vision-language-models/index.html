<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Evolution of Adversarial Robustness: From Neural Networks to Vision-Language Models | Binesh K Sadanandan </title> <meta name="author" content="Binesh K Sadanandan"> <meta name="description" content="Binesh Kumar, AI in Healthcare, Precision Oncology, Reinforcement Learning, Data Science. "> <meta name="keywords" content="binesh kumar, binesh, sadanandan, sail, university of new haven, medtronic"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bineshkumar.me/#%20the%20base%20hostname%20&amp;%20protocol%20for%20your%20site/blog/2025/adversarial-robustness-vision-language-models/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Binesh K Sadanandan </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/robmedllm-notes/">RobMedLLM Notes </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/research/">Research </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Evolution of Adversarial Robustness: From Neural Networks to Vision-Language Models</h1> <p class="post-meta"> May 22, 2025 • Binesh K Sadanandan </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/adversarial-ml"> <i class="fa-solid fa-hashtag fa-sm"></i> adversarial-ml</a>   <a href="/blog/tag/vision-language-models"> <i class="fa-solid fa-hashtag fa-sm"></i> vision-language-models</a>   <a href="/blog/tag/robustness"> <i class="fa-solid fa-hashtag fa-sm"></i> robustness</a>   <a href="/blog/tag/medical-ai"> <i class="fa-solid fa-hashtag fa-sm"></i> medical-ai</a>   <a href="/blog/tag/interpretability"> <i class="fa-solid fa-hashtag fa-sm"></i> interpretability</a>     ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   <a href="/blog/category/ai-safety"> <i class="fa-solid fa-tag fa-sm"></i> ai-safety</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Adversarial attacks expose hidden weaknesses in machine learning. This post traces how robustness research began with early neural networks and now extends to complex vision-language models. We highlight key discoveries, methods, and lessons for safe AI in healthcare.</p> <h2 id="1-early-insights-into-neural-networks-2014">1. Early Insights into Neural Networks (2014)</h2> <p><strong>Szegedy et al.</strong> first showed that deep nets have two surprising traits:</p> <ul> <li> <p><strong>Distributed knowledge</strong><br> Every neuron carries bits of meaning, so no single node holds a concept alone.</p> </li> <li> <p><strong>Adversarial examples</strong><br> Tiny, carefully chosen pixel changes can fool a classifier with high confidence.</p> </li> </ul> <p><strong>Main findings</strong>:</p> <ul> <li>Minimal perturbations (σ ≈ 0.06–0.14) yield 100% success against Inception models.</li> <li>Attacks transfer across models: 15–40% of examples that fool one network also fool others.</li> <li>The effect holds for linear models, convolutional nets, and unsupervised learners.</li> </ul> <p><strong>Why it happens</strong><br> High-dimensional input spaces behave almost linearly in many directions. With limited training data, models leave blind spots that adversaries can exploit.</p> <p><strong>Impact</strong><br> This work launched adversarial machine learning. A decade later, even advanced vision-language systems show the same core vulnerability.</p> <h2 id="2-robustness-in-vision-language-models-2023">2. Robustness in Vision-Language Models (2023)</h2> <p><strong>Zhao et al.</strong> studied how models like GPT-4 respond to image perturbations when accessed only via APIs.</p> <p><strong>Attack strategy</strong>:</p> <ol> <li> <strong>Transfer stage</strong><br> Generate initial adversarial images on a surrogate (CLIP or BLIP).</li> <li> <strong>Query refinement</strong><br> Fine-tune perturbations with a small number of black-box calls.</li> </ol> <p><strong>Key results</strong>:</p> <ul> <li>CLIP similarity scores jumped from 0.45 to over 0.80 using ε = 8/255 noise.</li> <li>Even small models (BLIP-Base, 224 M params) and large ones (MiniGPT-4, 14 B) were vulnerable.</li> <li>Only eight API queries were enough to reach high success rates.</li> </ul> <p><strong>Why it matters</strong><br> Vision attacks are invisible to humans and fully automated. Standard text defenses cannot block them.</p> <h2 id="3-text-only-deception-of-vlms-2025">3. Text-Only Deception of VLMs (2025)</h2> <p><strong>Ahn et al.</strong> introduced the <strong>MAC (Multimodal Adversarial Compositionality)</strong> framework. They used LLMs to craft text prompts that mislead vision-language models without changing images.</p> <p><strong>Framework highlights</strong>:</p> <ul> <li> <strong>Crossmodal similarity</strong><br> Ensure generated text scores highly when paired with the original image.</li> <li> <strong>Non-entailment</strong><br> Text contradicts the true image content.</li> <li> <strong>Lexical distance</strong><br> Keep word changes subtle but effective.</li> <li> <strong>Diversity</strong><br> Produce varied attack patterns to avoid detection.</li> </ul> <p><strong>Performance</strong>:</p> <ul> <li>Success rates rose from 6.9% (zero-shot) to 42.1% with self-training.</li> <li>Transfer attacks fooled 23–41% of other models.</li> <li>Only four self-training steps matched a 16-step zero-shot baseline.</li> </ul> <p><strong>Takeaway</strong><br> Even without touching pixels, clever text can exploit VLM weaknesses.</p> <h2 id="4-universal-surrogate-attacks-2025">4. Universal Surrogate Attacks (2025)</h2> <p><strong>Xu et al.</strong> showed that a single CLIP model can craft universal noise patterns that mislead many classifiers.</p> <p><strong>UnivIntruder method</strong>:</p> <ol> <li> <strong>Surrogate alignment</strong><br> Use CLIP text embeddings to guide perturbation direction.</li> <li> <strong>Feature-space tuning</strong><br> Target the embedding shifts that cause misclassification.</li> <li> <strong>Robust transformations</strong><br> Apply random crops and shifts to generalize perturbations.</li> </ol> <p><strong>Results</strong>:</p> <ul> <li>ImageNet top-1 attack rate: 85.1%</li> <li>CIFAR-10 attack rate: 99.4%</li> <li>Real-world services (Google, Baidu, GPT-4) saw 50–80% success.</li> <li>Required 80% fewer queries than previous black-box methods.</li> </ul> <p><strong>Defense notes</strong><br> Adversarial training helps but cuts clean accuracy and demands heavy compute. Test-time filters also degrade performance.</p> <h2 id="5-pitfalls-in-medical-imaging-2023">5. Pitfalls in Medical Imaging (2023)</h2> <p><strong>Ghamizi et al.</strong> reevaluated adversarial claims on chest X-ray classifiers and found common flaws:</p> <ul> <li> <strong>Binary labels only</strong><br> Most studies ignore multi-label disease patterns.</li> <li> <strong>Single dataset tests</strong><br> Lack of cross-hospital validation hides generalization issues.</li> <li> <strong>Unrealistic threat models</strong><br> White-box attacks assume full model access.</li> <li> <strong>Standard metrics fall short</strong><br> Plain accuracy misses risks from co-occurring conditions.</li> </ul> <p><strong>Key insights</strong>:</p> <ul> <li>PadChest models were three times more robust than NIH-trained ones (53.6% vs 13.8%).</li> <li>Risk-based attacks dropped robust accuracy from 32.1% to 11.1%.</li> <li>Data variety mattered more than model architecture for true robustness.</li> </ul> <p><strong>Lesson</strong><br> Healthcare demands domain-specific evaluations and realistic threat scenarios.</p> <h2 id="6-preserving-high-frequencies-in-vision-transformers-2024">6. Preserving High Frequencies in Vision Transformers (2024)</h2> <p><strong>Lin et al.</strong> identified that ViTs act like low-pass filters, losing fine details in diagrams and charts.</p> <p><strong>FM-ViT solution</strong>:</p> <ul> <li> <strong>Fourier split</strong><br> Separate image into low- and high-frequency components.</li> <li> <strong>Adaptive reweighting</strong><br> Learn weights for each frequency band per image.</li> <li> <strong>Wavelet guidance</strong><br> Use discrete wavelet transforms to set dynamic filters.</li> </ul> <p><strong>Impact</strong>:</p> <ul> <li>Significant accuracy gains on chart and diagram tasks.</li> <li>Better detection of small text and lines critical to medical scans.</li> </ul> <p><strong>Relevance</strong><br> In X-rays, preserving edges can make the difference between spotting a nodule or missing it.</p> <h2 id="7-interpreting-vlm-attention-2024">7. Interpreting VLM Attention (2024)</h2> <p><strong>Stan et al.</strong> released <strong>LVLM-Interpret</strong>, a tool to visualize how vision-language models allocate attention.</p> <p><strong>Features</strong>:</p> <ul> <li> <strong>Layer-by-layer maps</strong><br> Show token interactions across image and text.</li> <li> <strong>Relevancy scores</strong><br> Use gradients to rank which patches drive each token.</li> <li> <strong>Causal graphs</strong><br> Identify minimal image regions needed for a given output.</li> </ul> <p><strong>Findings</strong>:</p> <ul> <li>Text prompts sometimes dominate over visual cues.</li> <li>Five to ten patches often suffice to fix the model’s answer.</li> <li>Bias patterns emerged when prompts contained sensitive topics.</li> </ul> <p><strong>Use case</strong><br> Integrate this tool into clinical validation to catch unwanted text bias in medical VLMs.</p> <h2 id="8-token-activation-maps-2025">8. Token Activation Maps (2025)</h2> <p><strong>Yi Li et al.</strong> tackled the problem of early-token interference in multimodal generation.</p> <p><strong>TAM approach</strong>:</p> <ul> <li> <strong>Causal inference</strong><br> Estimate and remove influence from prior tokens.</li> <li> <strong>Gaussian denoising</strong><br> Filter out noise while keeping sharp details.</li> <li> <strong>Fusion maps</strong><br> Combine cleaned visual maps with textual relevance for each generated token.</li> </ul> <p><strong>Results</strong>:</p> <ul> <li>Up to 11-point F1‐IoU boost on COCO captions.</li> <li>Sharper object localization and clearer attribute highlighting.</li> </ul> <h2 id="conclusion-lessons-for-healthcare-ai">Conclusion: Lessons for Healthcare AI</h2> <ol> <li>Adversarial examples are a fundamental trait of high-dimensional models.</li> <li>Vision-language systems inherit and amplify these risks.</li> <li>Medical AI needs tailored threat models, multi-label tests, and cross-site validation.</li> <li>Frequency preservation and interpretability tools are vital for clinical trust.</li> <li>A unified evaluation across performance, safety, and fairness must guide deployment.</li> </ol> <p>By following these principles, we can build AI tools that serve patients safely and reliably.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/multimodal-llms-healthcare-applications/">Multimodal Large Language Models in Healthcare: Current Applications and Validation Approaches</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/mllmguard-safety-framework/">MLLMGuard: A Comprehensive Safety Framework for Multimodal Large Language Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/large-language-models-comprehensive-guide/">Large Language Models: From Architecture to Evaluation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/understanding-transformers-architecture/">Understanding the Transformer Architecture: A Deep Dive</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Binesh K Sadanandan. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?7254ae07fe9cc5f3a10843e1c0817c9c" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>