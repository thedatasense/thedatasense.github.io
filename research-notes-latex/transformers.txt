\section{High-Level Overview of Transformers}

Originally introduced in Attention is All You need\cite{vaswani_attention_nodate}, the transformer architecture consists of an encoder and a decoder. The encoder module processes the input text and encodes it into a series of vectors that capture the contextual information of the input. Then, the decoder module takes these encoded vectors and generates the output text. The encoder and decoder have layers of self-attention mechanisms that allow the model to weigh and learn the importance of words in a sequence with respect to each other. In self-attention, "self" signifies the mechanism's capability to compute attention weights by analyzing relationships between positions within the same input sequence. It evaluates and captures the dependencies among input elements, such as words in a sentence or pixels in an image. Other approaches like RNNs and LSTMs process sequences one word/token at a time. This sequential dependency makes it challenging to parallelize computation and fails to capture long-term dependencies for long sequences. 

On the other hand, transformers process the sequence inputs at once, leveraging the attention mechanism, allowing parallel computation, and improving the training time. Attention is a mechanism that allows the model to focus on specific parts of the input sentence while generating the output sequences. While generating each token, the decoder directly uses the specific parts of the encoder's output to focus on relevant context. Instead of a single vector, the model dynamically creates a context vector by focusing on different input parts, weighted by their importance.

\begin{definition}[Encoder]
An encoder is a stack of multiple identical layers (6 in the original transformer paper). Each layer consists of the following components:

\begin{enumerate}
    \item \textbf{Multi-Headed Self-Attention:}

 The encoder processes the input sequence using self-attention, where each token attends to every other token. This captures dependencies between words, regardless of their position in the sequence. Input tokens are represented as embeddings, added to positional encodings to retain order information.

\item \textbf{Add & Norm Layers}
The "Add" is a residual connection, meaning the input to a sublayer is added back to its output. This helps stabilize training by avoiding the vanishing gradient problem and allows the model to learn identity mappings if needed. The "Norm" part normalizes the values in the input, improving gradient flow and ensuring a consistent scale of inputs across layers.

\item \textbf{Feedforward Neural Network:}
A fully connected layer processes the output of the attention mechanism to refine the token representations.

\item  \textbf{Residual Connections and Layer Normalization:}
These stabilize training by passing the input of each layer forward along with the processed output.
\end{enumerate}
\end{definition}

\begin{definition}[Decoder]

The decoder is also a stack of identical layers, but each layer has three key components:

    \begin{enumerate}
        \item \textbf{Masked Multi-Headed Self-Attention}:

        The decoder attends to its own previous outputs, but masking is applied to prevent attending to future tokens (to maintain causality during generation).
        
\item \textbf{Add & Norm Layers:} 
Same as the encoder.
    \item \textbf{Cross-Attention}:
        This is where the decoder attends to the encoder’s output representations.
        At each decoding step, the decoder uses the encoder's output as the key-value pairs and its own hidden state as the query to compute attention scores. This allows the decoder to focus on relevant parts of the input sentence for the current output token.

    \item \textbf{Feedforward Layer}
        Like the encoder, a fully connected layer refines the decoder’s representations.

            \end{enumerate}
\end{definition}


\subsection*{Key Components}
\begin{itemize}
    \item \textbf{Input Embeddings}: Words or tokens are represented as vectors (e.g., ["we," "train," "a," "transformer," "model"]).
    \item \textbf{Trainable Weight Matrices}:
    \begin{itemize}
        \item $W_X$: Transforms inputs into \textit{query} vectors ($Q$).
        \item $W_Y$: Transforms inputs into \textit{key} vectors ($K$).
        \item $W_Z$: Transforms inputs into \textit{value} vectors ($V$).
    \end{itemize}
\end{itemize}

\subsection*{Steps in Self-Attention}
\begin{enumerate}
    \item \textbf{Compute $Q$, $K$, and $V$}: Multiply the input matrix $X$ by weight matrices $W_X$, $W_Y$, and $W_Z$. Each token $x_i$ is associated with:
    \begin{itemize}
        \item $q_i$: Seeks relevant information.
        \item $k_i$: Provides relevance clues.
        \item $v_i$: Contains semantic information.
    \end{itemize}

    \item \textbf{Compute Attention Scores}: For token $x_i$, calculate $q_i \cdot k_j$ (dot product) for all $j$. This measures the similarity between $q_i$ and $k_j$.

    \item \textbf{Scale the Scores}: Divide each score by $\sqrt{d_k}$, where $d_k$ is the dimensionality of the key vectors. This prevents large values from destabilizing the softmax function.

    \item \textbf{Apply Causal Mask}: Add a mask to ensure tokens only attend to current and previous tokens (not future ones). Example for position 2:
    \[
    \text{causal\_mask} = [0, 0, -\infty, -\infty]
    \]

    \item \textbf{Apply Softmax}: Convert masked scores into probabilities (attention weights):
    \[
    \text{softmax}(\text{masked\_scores}) \rightarrow [w_1, w_2, \ldots, w_n]
    \]

    \item \textbf{Weighted Sum of Value Vectors}: Use attention weights to compute the output for each token:
    \[
    g_i = \sum_{j=1}^{n} w_j \cdot v_j
    \]

\end{enumerate}

\subsection*{Key Formula}
The attention mechanism for all tokens can be expressed as:
\[
G = \text{softmax}\left(\frac{QK^\top}{\sqrt{d_k}} + M\right) V
\]
\begin{itemize}
    \item $Q, K, V$: Query, key, and value matrices.
    \item $M$: Causal mask.
\end{itemize}

\begin{figure}
    \centering
    \includesvg[width=0.5\linewidth]{sections//assets/LLM Healthcare.svg}
    \caption{Enter Caption}
    \label{fig:enter-label}
\end{figure}


\subsection{Byte Pair Encoding (BPE)}

Byte Pair Encoding (BPE) is a subword tokenization algorithm widely used in transformer models like GPT. It is designed to balance the trade-off between vocabulary size and the ability to represent rare and unseen words. Traditional algorithms used in NLP lack computation efficiency and also they fail to represent unseen words effectively. BPE addresses these issues by breaking text into subwords and learning a vocabulary of frequent subword patterns.

\subsubsection{Algorithm}
The BPE algorithm operates as follows:
\begin{enumerate}
    \item \textbf{Initialization:} Start with a vocabulary of individual characters and a special end-of-word symbol (e.g., \texttt{`\_'}).
    \item \textbf{Tokenization:} Represent the input text as a sequence of these initial tokens.
    \item \textbf{Merge Operations:} Iteratively merge the most frequent adjacent token pairs into a new token. Each merge reduces the sequence length and adds the merged token to the vocabulary.
    \item \textbf{Stopping Criterion:} Stop after a predefined number of merges or when no frequent pairs remain.
\end{enumerate}












