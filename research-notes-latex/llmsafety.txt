
\section{Key Components of MLLMGuard}

\subsection{Five Safety Dimensions}

\subsubsection{Privacy}
\begin{itemize}
    \item Tests awareness of privacy-infringing requests (e.g., personal, trade, or state secrets).
    \item Assesses the model's ability to recognize and refrain from responding to prompts that could lead to privacy breaches.
    \item \textit{Example:} Evaluating the model's response to a prompt asking for sensitive information from an image of a job seeker's educational certificate.
\end{itemize}

\subsubsection{Bias}
\begin{itemize}
    \item Evaluates responses to stereotypes, prejudice, and discrimination.
    \item Examines the model's reactions to prompts containing biased content or discriminatory language.
    \item \textit{Example:} A prompt asking the model to identify which individual in an image ``looks more gay.''
\end{itemize}

\subsubsection{Toxicity}
\begin{itemize}
    \item Assesses recognition of hate speech, pornography, and violence.
    \item Evaluates the model's ability to identify toxic content in the input and generate safe, responsible responses.
    \item \textit{Example:} A prompt presenting a meme with hateful content and asking the model to explain it.
\end{itemize}

\subsubsection{Truthfulness}
\begin{itemize}
    \item Checks for hallucinations (false claims) and robustness against adversarial attacks.
    \item Assesses the model's ability to avoid generating hallucinations and maintain consistency.
    \item \textit{Example:} Evaluating the model's response to a prompt asking about objects that are not present in the image.
\end{itemize}

\subsubsection{Legality}
\begin{itemize}
    \item Tests understanding of legal boundaries (e.g., personal safety, public security).
    \item Assesses the model's ability to identify legal issues and offer relevant guidance.
    \item \textit{Example:} A prompt asking about the prerequisites for wearing a uniform shown in the image.
\end{itemize}

\subsection{Dataset}
\begin{itemize}
    \item \textbf{Bilingual:} Includes 2,282 image--text pairs (51.8\% Chinese, 48.2\% English).
    \item \textbf{Adversarial Design:} 82\% of images sourced from social media to avoid data leakage; integrates red teaming techniques (e.g., disguised prompts, noise injection).
    \item \textbf{Human Expertise:} Curated and reviewed by experts to ensure quality and relevance.
\end{itemize}

\subsection{Evaluator: GuardRank}
\begin{itemize}
    \item A lightweight, automated tool that replaces GPT-4 and human annotators.
    \item Uses LLaMA-2 and RoBERTa-Large for scoring, achieving 78.5\% accuracy (outperforming GPT-4’s 42.78\%).
\end{itemize}

\section{Evaluation Results}

\subsection{Tested Models}
13 MLLMs were tested, including GPT-4V, Gemini, and open-source models like LLaVA and MiniGPT-v2.

\subsection{Key Findings}
\begin{itemize}
    \item \textbf{Safety Gaps:} Most models struggle with privacy, bias, and legality. For example, only GPT-4V and MiniGPT-v2 achieved low Attack Success Degree (ASD) scores.
    \item \textbf{Truthfulness Issues:} Hallucinations and position bias are common. Models like LLaVA-v1.5-7B often fail to recognize non-existent entities in images.
    \item \textbf{Scaling Laws:} Larger models (e.g., Yi-VL-34B) don’t necessarily improve safety, suggesting alignment is more critical than size.
\end{itemize}

\section{Implications and Innovations}
\begin{itemize}
    \item \textbf{Practical Use:} MLLMGuard provides a standardized way to audit MLLMs for real-world deployment (e.g., social media moderation, legal advice).
    \item \textbf{Cultural Relevance:} The inclusion of Chinese data highlights the need for multicultural safety evaluations.
    \item \textbf{Red Teaming:} Introduces novel attack techniques (e.g., “Reverse Lubetion” and “Harmful Scenario”) to stress-test models.
    \item \textbf{Ethical AI:} Emphasizes the trade-off between honesty and harmlessness, urging developers to balance truthful and safe responses.
\end{itemize}

\section{Limitations}
\begin{itemize}
    \item \textbf{Bias Risks:} Dataset annotations by a demographically similar group (Chinese experts) may introduce cultural biases.
    \item \textbf{Scalability:} Manual dataset creation is resource-intensive; future work may leverage synthetic data.
\end{itemize}