\documentclass[10pt]{article}
\usepackage[margin=0.75in]{geometry}
\usepackage{amsmath,amssymb}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{enumitem}
\setlist{nosep}

% Compact formatting
\setlength{\parindent}{0pt}
\setlength{\parskip}{0.5em}

% Define consistent color scheme
\definecolor{maincolor}{RGB}{0,51,102}
\definecolor{accentcolor}{RGB}{204,0,0}

\begin{document}

\begin{center}
{\Large \textbf{\color{maincolor}Intriguing Properties of Neural Networks (2014)}}\\
\vspace{0.2em}
{\small Szegedy, Zaremba, Sutskever, Bruna, Erhan, Goodfellow, Fergus}
\end{center}

\vspace{-0.5em}
\begin{tcolorbox}[colback=blue!5,colframe=maincolor,boxrule=0.5pt]
\textbf{Core Discovery:} Neural networks have two counterintuitive properties that fundamentally challenge our understanding: (1) semantic information is distributed across all neurons rather than localized, and (2) imperceptibly small perturbations can fool networks into misclassifying with high confidence.
\end{tcolorbox}

\textbf{\color{maincolor}The Two Properties}

\textbf{1. Distributed Representations:} The paper showed that random directions in a network's activation space are just as semantically meaningful as individual neurons. When visualizing what maximizes $\langle\phi(x), v\rangle$ for random vector $v$, the resulting images share coherent semantic properties. This means networks don't learn human-interpretable "grandmother cells" but rather distribute information across the entire layer.

\textbf{2. Adversarial Examples:} By solving the optimization problem $\min \|r\|_2$ subject to $f(x+r) = l_{target}$, the authors could create perturbations invisible to humans that cause misclassification. Remarkably, these adversarial examples \textit{transfer} between different models:even those trained on completely different datasets.

\textbf{\color{maincolor}Key Experimental Findings}

The experiments revealed surprising robustness failures across all tested architectures:
\begin{itemize}
\item \textbf{Perturbation magnitude:} Only $\sigma = 0.058-0.14$ needed for 100\% attack success (vs. random noise at $\sigma = 0.1$ achieving just 51\% error)
\item \textbf{Transfer rates:} 15-40\% of adversarial examples fooled models with different architectures; 5-27\% transferred across disjoint training sets
\item \textbf{Universality:} Phenomenon observed in linear models, fully connected networks, CNNs (AlexNet), and massive unsupervised models
\end{itemize}

\textbf{\color{maincolor}Why This Happens}

The paper suggests adversarial examples exist due to the high-dimensional geometry of neural networks. In high dimensions, networks behave approximately linearly in most directions, and there isn't enough training data to constrain behavior in all possible directions around each sample. The Lipschitz analysis showed instabilities can emerge from the very first layer.

\textbf{\color{maincolor}Lasting Impact}

This paper created the field of adversarial machine learning and revealed a fundamental vulnerability that persists today. Ten years later, modern vision-language models (VLMs) like CLIP, BLIP, and LLaVA still exhibit these vulnerabilities, with recent work showing adversarial images can fool models across thousands of different text prompts. The transferability property suggests this isn't overfitting but a fundamental characteristic of how neural networks learn.

\textbf{\color{maincolor}Critical Implications}

\begin{enumerate}
\item \textbf{Safety:} Any deployment of neural networks in critical systems must account for adversarial vulnerabilities
\item \textbf{Understanding:} Networks don't learn representations the way we intuitively expect
\item \textbf{Evaluation:} Accuracy alone is insufficient; robustness must be explicitly tested
\item \textbf{Theory:} The smooth interpolation we expect from generalization coexists with extreme local sensitivity
\end{enumerate}

\textbf{\color{accentcolor}Open Questions:} Why do adversarial examples transfer so well? Are there fundamentally more robust architectures? How do biological vision systems handle similar perturbations?

\vspace{0.3em}
\hrule
\vspace{0.2em}
{\footnotesize \textbf{Key Insight:} This paper exemplifies transformative research:simple experiments revealing that our most successful models have properties completely at odds with our intuitions about how they work.}\newpage

\begin{center}
{\Large \textbf{\color{maincolor}On Evaluating Adversarial Robustness of Large Vision-Language Models (2023)}}\\
\vspace{0.2em}
{\small Zhao, Pang, Du, Yang, Li, Cheung, Lin}
\end{center}

\vspace{-0.5em}
\begin{tcolorbox}[colback=blue!5,colframe=maincolor,boxrule=0.5pt]
\textbf{Core Discovery:} Large Vision-Language Models (VLMs) like GPT-4, despite their impressive capabilities, can be fooled into generating targeted responses through imperceptible image perturbations:even under realistic black-box constraints where attackers only have API access.
\end{tcolorbox}

\textbf{\color{maincolor}The Attack Framework}

\textbf{1. Two-Stage Pipeline:} The authors develop a sophisticated attack combining transfer-based and query-based strategies. First, they craft adversarial examples using surrogate models (CLIP/BLIP) through either image-text matching (MF-it) or image-image matching (MF-ii). Then, they refine these examples using black-box queries with Random Gradient-Free (RGF) optimization.

\textbf{2. Cross-Modal Vulnerability:} Unlike text-based attacks requiring extensive prompt engineering, vision attacks can be automated and remain imperceptible. The adversarial effects persist across multi-round conversations, fundamentally compromising the entire interaction.

\textbf{\color{maincolor}Key Experimental Findings}

The evaluation across 6 state-of-the-art VLMs revealed alarming vulnerabilities:
\begin{itemize}
\item \textbf{Attack success rates:} CLIP scores increase from 0.4-0.5 (clean) to 0.8+ (adversarial) with just $\epsilon = 8/255$ perturbations
\item \textbf{Transfer effectiveness:} MF-ii consistently outperforms MF-it; combining with queries (MF-ii + MF-tt) achieves highest success
\item \textbf{Model vulnerability:} All tested models susceptible:from lightweight BLIP (224M) to massive MiniGPT-4 (14.1B parameters)
\item \textbf{Query efficiency:} Just 8 additional black-box queries significantly boost attack performance
\end{itemize}

\textbf{\color{maincolor}Why This Matters}

VLMs are being deployed in safety-critical applications:medical diagnosis, autonomous systems, robotics. The vision modality presents a more severe attack surface than text because: (1) perturbations are imperceptible to humans, (2) attacks can be fully automated without human-in-the-loop, and (3) defenses developed for text don't transfer to vision.

\textbf{\color{maincolor}Technical Innovation}

The paper's key insight is using text-to-image models (Stable Diffusion) to generate target images for any arbitrary text, enabling flexible adversarial scenarios. This removes the constraint of needing paired image-text data and allows attackers to craft targeted responses on demand.

\textbf{\color{maincolor}Broader Implications}

\begin{enumerate}
\item \textbf{Deployment risk:} Open-source VLMs are being integrated as plugins without adequate security evaluation
\item \textbf{Asymmetric vulnerability:} Vision attacks are stealthier and more automatable than text attacks
\item \textbf{Defense gap:} No existing defenses specifically designed for multimodal architectures
\item \textbf{Evaluation necessity:} Adversarial robustness testing must become standard for VLM deployment
\end{enumerate}

\textbf{\color{accentcolor}Open Questions:} Can we develop multimodal defenses that ensure cross-modal consistency? Do larger models naturally become more robust? How do these digital attacks translate to physical-world scenarios?

\vspace{0.3em}
\hrule
\vspace{0.2em}
{\footnotesize \textbf{Key Insight:} A decade after adversarial examples were discovered, multimodal models have inherited and amplified these vulnerabilities, creating new attack surfaces that bypass the relative transparency of language-based interactions.}

\newpage

\begin{center}
{\Large \textbf{\color{maincolor}Can LLMs Deceive CLIP? Benchmarking Adversarial Compositionality of Pre-trained Multimodal Representations (2025)}}\\
\vspace{0.2em}
{\small Ahn, Yun, Ko, Kim}
\end{center}

\vspace{-0.5em}
\begin{tcolorbox}[colback=blue!5,colframe=maincolor,boxrule=0.5pt]
\textbf{Core Discovery:} Large Language Models can systematically expose compositional vulnerabilities in Vision-Language Models by generating deceptive text that maintains high crossmodal similarity while fundamentally contradicting the original content:achieving up to 42\% attack success rate on CLIP using only text modifications.
\end{tcolorbox}

\textbf{\color{maincolor}The MAC Framework}

\textbf{1. Multimodal Adversarial Compositionality (MAC):} A comprehensive benchmark that evaluates how effectively LLMs can deceive pre-trained multimodal representations. Unlike prior work limited to specific modalities or attack types, MAC provides:
\begin{itemize}
\item \textbf{Modality-agnostic evaluation:} Works across images, videos, and audio
\item \textbf{Four-criteria assessment:} Crossmodal similarity, unimodal non-entailment, lexical distance, and auxiliary constraints
\item \textbf{Diversity metrics:} Entropy-based measures to ensure varied attack patterns
\end{itemize}

\textbf{2. Self-Training with Diversity Promotion:} The authors develop a novel approach combining rejection sampling fine-tuning (RFT) with diversity-promoting filtering:
\begin{itemize}
\item Generate N candidates using LLMs (e.g., Llama-3.1-8B)
\item Filter samples meeting all attack criteria
\item Apply Gibbs sampling to select diverse successful attacks
\item Fine-tune LLM on selected samples for improved generation
\end{itemize}

\textbf{\color{maincolor}Key Experimental Findings}

Evaluation across 3 modalities and 6 state-of-the-art models revealed:
\begin{itemize}
\item \textbf{Dramatic ASR improvement:} From 6.88\% (zero-shot) to 42.10\% (with self-training) on COCO/CLIP
\item \textbf{High transferability:} Attacks transfer between models with 23-41\% success rate
\item \textbf{Diversity enhancement:} Entropy increased from 7.507 to 7.747 with diversity promotion
\item \textbf{Efficiency gains:} Self-trained models with N=4 match zero-shot performance at N=16
\end{itemize}

\textbf{\color{maincolor}Attack Mechanism}

The attack exploits VLMs' compositional understanding through minimal text perturbations:
\begin{enumerate}
\item \textbf{Crossmodal deception:} Generated text scores higher similarity than ground truth
\item \textbf{Semantic contradiction:} Changes create non-entailing content (e.g., "reaching for keys" → "accidentally typing email")
\item \textbf{Lexical constraints:} Average edits limited to < L/2 tokens
\item \textbf{Diverse patterns:} Avoids repetitive attacks through entropy maximization
\end{enumerate}

\textbf{\color{maincolor}Technical Innovation}

\begin{itemize}
\item \textbf{Best-of-N strategy:} Balances attack success with computational efficiency
\item \textbf{Multi-round training:} Iterative improvement reaching saturation by round 3
\item \textbf{Attribute-enriched tokens:} Track operation types (insert/delete), POS tags, and lemmas for diversity analysis
\item \textbf{Unified evaluation:} First framework to systematically compare rule-based, human, and LLM-based attack methods
\end{itemize}

\textbf{\color{maincolor}Broader Implications}

\begin{enumerate}
\item \textbf{Universal vulnerability:} All tested VLMs susceptible, from lightweight BLIP (224M) to massive models
\item \textbf{Healthcare risks:} Critical for medical imaging where misalignment could impact diagnosis
\item \textbf{Defense inadequacy:} Text-based defenses don't transfer to multimodal attacks
\item \textbf{Benchmark necessity:} MAC evaluation should become standard for VLM deployment
\end{enumerate}

\textbf{\color{accentcolor}Key Limitations:} Focus on short captions; assumes API access to target models; computational cost of self-training; potential for generating biased content.

\textbf{\color{accentcolor}Open Questions:} Can architectural changes fundamentally improve robustness? How do these digital attacks translate to real-world scenarios? What defenses ensure cross-modal consistency?

\vspace{0.3em}
\hrule
\vspace{0.2em}
{\footnotesize \textbf{Key Insight:} This work reveals that the multimodal binding in VLMs is surprisingly fragile:models that appear to understand image-text relationships can be systematically fooled by subtle linguistic manipulations, highlighting a fundamental gap between pattern matching and true compositional understanding.}

\begin{center}
{\Large \textbf{\color{maincolor}One Surrogate to Fool Them All: Universal, Transferable, and Targeted Adversarial Attacks with CLIP (2025)}}\\
\vspace{0.2em}
{\small Xu, Dai, Tang, Zhang}
\end{center}

\vspace{-0.5em}
\begin{tcolorbox}[colback=blue!5,colframe=maincolor,boxrule=0.5pt]
\textbf{Core Discovery:} A single publicly available CLIP model can generate universal adversarial perturbations that force arbitrary DNNs to misclassify images into attacker-specified target classes:achieving up to 85\% ASR on ImageNet and successfully compromising real-world services like Google Image Search (84\%) and GPT-4 (80\%) without any access to target models.
\end{tcolorbox}

\textbf{\color{maincolor}The UnivIntruder Framework}

\textbf{1. Three Key Innovations:} UnivIntruder addresses fundamental challenges in transferable attacks:
\begin{itemize}
\item \textbf{CLIP-based surrogate model:} Uses textual concepts (target/negative classes) to align with unknown target models
\item \textbf{Feature direction mechanism:} Captures perturbation effects in embedding space to counter dataset biases
\item \textbf{Robust random transformations:} Applies differentiable transformations to prevent overfitting to surrogate structure
\end{itemize}

\textbf{2. Attack Pipeline:} The method operates in two stages:
\begin{enumerate}
\item \textbf{Surrogate Model Generation:} Builds CLIP-based model using:
   - Image direction: $D_x = E_I(\hat{x}) - E_I(x)$ (perturbed minus clean embeddings)
   - Text directions: $D_{y_t} = E_T(y_t) - E_T(y)$ (target minus source concept)
   - Similarity optimization: $\text{sim}(D_x, D_{y_t})$ maximized, $\text{sim}(D_x, D_{Y_n})$ minimized
\item \textbf{Robust Attack:} Applies differentiable transformations (rotation, scaling, patching) during optimization to enhance transferability
\end{enumerate}

\textbf{\color{maincolor}Key Experimental Findings}

Attack performance across datasets and real-world applications:
\begin{itemize}
\item \textbf{Dataset ASRs:} CIFAR-10 (99.4\%), CIFAR-100 (98.19\%), Caltech-101 (92.1\%), ImageNet (85.1\% top-1, 95.4\% top-5)
\item \textbf{Real-world services:} Google (53\%), Baidu (84\%), Claude-3.5 (80\%), GPT-4 (64\%)
\item \textbf{Query reduction:} Reduces black-box attack queries by 80\% when used as initialization
\item \textbf{Transfer rates:} 23-41\% cross-model transfer without target model access
\end{itemize}

\textbf{\color{maincolor}Technical Innovation}

\begin{itemize}
\item \textbf{Direction-based alignment:} Using embedding differences rather than raw embeddings eliminates dataset biases
\item \textbf{Text template ensemble:} Averages multiple phrasings ("a photo of [X]", "a blurry [X]") for robust text embeddings
\item \textbf{Best-of-N strategy:} Generates N=4 candidates, selecting best based on attack criteria
\item \textbf{Universal perturbation:} Single perturbation $\mathcal{T}$ works across all images in dataset
\end{itemize}

\textbf{\color{maincolor}Why This Attack Succeeds}

\begin{enumerate}
\item \textbf{CLIP's broad knowledge:} Pre-trained on billions of image-text pairs, CLIP encapsulates visual concepts needed for various tasks
\item \textbf{Semantic alignment:} Text concepts provide semantic guidance that transfers better than pixel-level patterns
\item \textbf{Robust optimization:} Random transformations prevent overfitting to specific model architectures
\item \textbf{Feature space attack:} Operating in embedding space rather than pixel space improves transferability
\end{enumerate}

\textbf{\color{maincolor}Defense Analysis}

The paper evaluates various defenses:
\begin{itemize}
\item \textbf{Adversarial training:} Reduces ASR to 15-30\% but requires expensive retraining
\item \textbf{Test-time defenses:} DiffPure, IG-Defense reduce ASR but significantly degrade clean accuracy
\item \textbf{Robustness detection:} Feature robustness scores overlap between clean and perturbed images
\item \textbf{Concept protection:} Label obfuscation provides minimal protection (<5\% ASR reduction)
\end{itemize}

\textbf{\color{accentcolor}Practical Implications:} Real-world AI services remain vulnerable to universal attacks crafted with only public models and text labels:no target model access needed.

\textbf{\color{accentcolor}Key Limitations:} Performance varies by architecture (ViT/Swin more resistant); requires CLIP-like pretrained model; some semantic concepts harder to target; perturbations may be visible at lower $\ell_\infty$ bounds.

\vspace{0.3em}
\hrule
\vspace{0.2em}
{\footnotesize \textbf{Key Insight:} This work demonstrates that the semantic knowledge in publicly available vision-language models can be weaponized to create highly transferable attacks, highlighting a fundamental vulnerability where general-purpose models become tools for compromising specific task-focused systems.}

\begin{center}
{\Large \textbf{\color{maincolor}On Evaluating Adversarial Robustness of Chest X-ray Classification: Pitfalls and Best Practices (2023)}}\\
\vspace{0.2em}
{\small Ghamizi, Cordy, Papadakis, Le Traon}
\end{center}

\vspace{-0.5em}
\begin{tcolorbox}[colback=blue!5,colframe=maincolor,boxrule=0.5pt]
\textbf{Core Discovery:} Previous claims of 100\% attack success rates on chest X-ray classifiers are oversimplified. Robustness evaluation requires careful consideration of medical domain peculiarities: multi-label classification, labeler disagreement, disease co-occurrence, and clinical risk implications.
\end{tcolorbox}

\textbf{\color{maincolor}Context and Motivation}

Chest X-ray classification has been a prime target for automated diagnosis systems, with deep learning models achieving radiologist-level performance. However, the adversarial robustness of these systems remains poorly understood. While adversarial attacks on natural images (ImageNet, CIFAR) are well-studied, medical imaging presents unique challenges that invalidate standard evaluation protocols.

\textbf{\color{maincolor}Key Methodological Insights}

\textbf{1. Multi-label Complexity:} Unlike ImageNet's single-label classification, chest radiographs commonly show multiple concurrent diseases. The paper introduces \textit{k-robust accuracy} to account for this:measuring how many of the top-k predicted labels match ground truth labels.

\textbf{2. Cross-Domain Vulnerability:} Models exhibit drastically different robustness depending on training and test distributions. The NIH model shows 15.4\% robust accuracy (1-robust) while CHEX drops to 1.6\% under identical attack conditions.

\textbf{3. Risk-Aware Attacks:} The authors develop targeted attacks using disease co-occurrence matrices. Attacks aim for clinically risky misclassifications (e.g., Pneumothorax $\rightarrow$ Infiltration) rather than arbitrary errors, better modeling real-world threats.

\textbf{\color{maincolor}Experimental Findings}

The evaluation across 3 datasets, 7 models, and 18 diseases reveals:
\begin{itemize}
\item \textbf{Dataset-dependent robustness:} PadChest examples are 3× more robust than NIH examples (53.6\% vs 13.8\% 3-robust accuracy)
\item \textbf{Architecture matters less than data:} DenseNet and ResNet show similar vulnerabilities when trained on the same dataset
\item \textbf{Transfer attacks succeed but vary:} 15-40\% of adversarial examples transfer between architectures
\item \textbf{Risk-based attacks more effective:} Targeting improbable disease combinations reduces robustness from 32.1\% to 11.1\% (AllD model, k=3)
\end{itemize}

\textbf{\color{maincolor}Critical Pitfalls Identified}

\begin{enumerate}
\item \textbf{Binary classification bias:} 9/16 prior studies only evaluated binary (normal vs. disease) classification, missing multi-label complexity
\item \textbf{Single dataset evaluation:} All prior work used one dataset, missing cross-domain generalization failures
\item \textbf{Inadequate threat models:} Most assumed white-box access, unrealistic for medical systems
\item \textbf{Weak baselines:} Only 2/16 studies evaluated adversarially trained models
\item \textbf{Inappropriate metrics:} Standard accuracy ignores disease co-occurrence and clinical risk
\end{enumerate}

\textbf{\color{maincolor}Dissertation Relevance}

This work directly addresses VLM robustness evaluation methodology:crucial for multimodal medical AI. Key takeaways for VLM research:
\begin{itemize}
\item Domain-specific evaluation metrics are essential (k-robust accuracy, risk scores)
\item Cross-dataset evaluation reveals true generalization limits
\item Medical VLMs require threat models accounting for clinical deployment constraints
\item The vision modality in VLMs may inherit these CXR vulnerabilities
\end{itemize}

\textbf{\color{maincolor}Future Research Directions}

\begin{enumerate}
\item \textbf{Multimodal robustness:} Extend evaluation to vision-language models combining CXR + radiology reports
\item \textbf{Physical perturbations:} Evaluate robustness to real-world image artifacts (motion blur, exposure variations)
\item \textbf{Certified defenses:} Develop provable robustness guarantees for safety-critical medical deployment
\item \textbf{Cross-modal consistency:} Ensure vision and text modalities provide consistent predictions under attack
\end{enumerate}

\textbf{\color{accentcolor}Open Questions:} How do these vulnerabilities manifest in multimodal architectures? Can language supervision improve vision robustness? What constitutes "acceptable" robustness for clinical deployment?

\vspace{0.3em}
\hrule
\vspace{0.2em}
{\footnotesize \textbf{Key Insight:} Medical AI robustness cannot be evaluated using natural image protocols. Domain expertise must inform threat models, metrics, and acceptable performance thresholds:a lesson critical for healthcare VLM deployment.}

\begin{center}
{\Large \textbf{\color{maincolor}Non-Natural Image Understanding with Advancing Frequency-based Vision Encoders (2024)}}\\
\vspace{0.2em}
{\small Lin, Wang, Feng, Wang, Jin, Zhao, Wu, Yao, Chen}
\end{center}

\vspace{-0.5em}
\begin{tcolorbox}[colback=blue!5,colframe=maincolor,boxrule=0.5pt]
\textbf{Core Discovery:} Vision Transformers (ViTs) in MLLMs act as low-pass filters, progressively losing high-frequency information crucial for non-natural images. FM-ViT introduces frequency modulation to preserve these details, achieving state-of-the-art performance on geometric, chart, and function understanding tasks.
\end{tcolorbox}

\textbf{\color{maincolor}The Frequency Problem}

\textbf{1. Low-Pass Filter Behavior:} Through Fourier analysis, the authors demonstrate that self-attention in ViTs systematically reduces high-frequency components as layers deepen. The $\Delta \log$ amplitude at high frequency (1.0$\pi$) drops significantly: from -0.50 at layer 0 to -1.50 at layer 20, indicating substantial information loss.

\textbf{2. Impact on Non-Natural Images:} Unlike natural images where low-frequency features (textures, colors) dominate, non-natural images concentrate semantic information in high-frequency components:edges, lines, angles, and geometric structures. Standard ViTs fail to capture these critical elements, explaining poor performance on charts, diagrams, and mathematical figures.

\textbf{\color{maincolor}The FM-ViT Solution}

The paper introduces frequency modulation directly into the self-attention mechanism:
\begin{itemize}
\item \textbf{Fourier Decomposition:} Separates attention features $A$ into direct current $A_D$ and high-frequency $A_H$ components
\item \textbf{Learnable Re-weighting:} $A = w_d A_H + w_h A_D$ where $w_d$, $w_h$ are trainable parameters
\item \textbf{Sample-wise Adaptation:} Uses Discrete Wavelet Transform (DWT) to dynamically adjust weights based on input characteristics
\item \textbf{Three-stage Training:} (1) Contrastive learning for vision-language alignment, (2) LLM integration, (3) Instruction tuning
\end{itemize}

\textbf{\color{maincolor}Key Experimental Results}

FM-ViT demonstrates substantial improvements across all non-natural image tasks:
\begin{itemize}
\item \textbf{Classification accuracy:} Geometric 61.5\% (vs. CLIP ViT-L 57.9\%), Charts 92.1\% (vs. 91.6\%), Functions 65.9\% (vs. 62.6\%)
\item \textbf{Question answering:} GeoQA 68.2\% (outperforming G-LLaVA 64.2\%), FunctionQA 43.5\% (best open-source), ChartQA 72.5\% (surpassing ChartLlama 69.6\%)
\item \textbf{Caption quality:} Approaches GPT-4V performance on geometric tasks (2.35 vs. 2.33 correctness)
\item \textbf{Frequency preservation:} Maintains high-frequency information across all layers, unlike standard ViT
\end{itemize}

\textbf{\color{maincolor}Technical Innovation}

The key insight is treating different image types as having distinct frequency signatures. The adaptive mechanism learns:
$$\hat{w}_d = \sigma(\text{FC}(\hat{c})), \quad \hat{w}_h = \sigma(\text{FC}(\hat{c}))$$
where $\hat{c}$ contains wavelet-derived statistics, enabling dynamic adjustment without manual tuning.

\textbf{\color{maincolor}Relevance to Medical Imaging}

This work has profound implications for chest X-ray analysis where high-frequency details are critical:

\begin{enumerate}
\item \textbf{Anatomical boundaries:} Pleural lines, cardiac silhouettes, diaphragm edges
\item \textbf{Subtle pathologies:} Early infiltrates, small nodules, minimal pneumothorax
\item \textbf{Medical devices:} Lines, tubes, and wires:essentially geometric elements
\item \textbf{Texture patterns:} Interstitial markings, ground-glass opacities, reticular patterns
\end{enumerate}

\textbf{\color{maincolor}Broader Implications for Healthcare AI}

\begin{itemize}
\item \textbf{Diagnostic accuracy:} Preserving high-frequency information could reduce missed findings in automated screening
\item \textbf{Robustness:} Sample-wise adaptation could handle varying image quality (portable vs. standard radiographs)
\item \textbf{Generalization:} Technique applicable to other medical imaging modalities (CT, MRI, ultrasound)
\item \textbf{Trust:} Better preservation of diagnostic details could increase clinician confidence in AI systems
\end{itemize}

\textbf{\color{accentcolor}Open Questions:} How do frequency characteristics differ across medical imaging modalities? Can domain-specific frequency priors improve performance? What's the optimal balance between computational cost and diagnostic accuracy in clinical settings?

\vspace{0.3em}
\hrule
\vspace{0.2em}
{\footnotesize \textbf{Key Insight:} By recognizing that information distribution varies fundamentally between image types, FM-ViT addresses a critical blind spot in current VLMs:one particularly detrimental to medical imaging where subtle, high-frequency details often determine diagnosis.}
\begin{center}
{\Large \textbf{\color{maincolor}LVLM-Interpret: An Interpretability Tool for Large Vision-Language Models (2024)}}\\
\vspace{0.2em}
{\small Stan, Aflalo, Rohekar, Bhiwandiwalla, Tseng, Olson, Gurwicz, Wu, Duan, Lal}
\end{center}

\vspace{-0.5em}
\begin{tcolorbox}[colback=blue!5,colframe=maincolor,boxrule=0.5pt]
\textbf{Core Discovery:} Large Vision-Language Models exhibit interpretable attention patterns that reveal systematic biases:particularly a tendency to over-rely on text prompts rather than visual content, making them vulnerable to manipulation through carefully crafted queries.
\end{tcolorbox}

\textbf{\color{maincolor}The Setup}

As vision-language models like GPT-4V and LLaVA become integral to healthcare applications (medical imaging, diagnostic support), understanding their decision-making becomes critical. These models combine massive parameter counts with multimodal processing, yet remain black boxes prone to hallucination:generating plausible but incorrect outputs. Existing interpretability tools weren't designed for the unique challenges of multimodal architectures, creating a gap between deployment ambitions and safety requirements.

\textbf{\color{maincolor}The Breakthrough}

LVLM-Interpret provides an interactive interface that combines three complementary interpretability methods: raw attention visualization, relevancy mapping, and causal graph analysis. The tool revealed that LLaVA systematically prioritizes textual cues over visual evidence in certain scenarios, explaining previously mysterious failure modes. Most surprisingly, the causal analysis (CLEANN) demonstrated that these models implicitly learn structured causal relationships between image patches and output tokens.

\textbf{\color{maincolor}The Blueprint}

\textbf{Architecture:} Browser-based Gradio interface compatible with any transformer-based LVLM. Users upload images, issue queries, and explore model internals through multiple lenses.

\textbf{Core Methods:}
\begin{itemize}
\item \textbf{Layer Attention Maps:} Visualize token-to-token interactions across modalities, selectable by layer/head
\item \textbf{Relevancy Scores:} Backward propagate importance through both LLM and vision encoder using gradient-based attribution
\item \textbf{Causal Graphs:} Learn partial ancestral graphs from attention matrices to identify minimal explanatory image patches
\end{itemize}

\textbf{Evaluation:} Tested on LLaVA-v1.5-7b using MMVP benchmark:a dataset of "CLIP-blind" images designed to expose LVLM weaknesses. Analysis focused on cases where identical images received contradictory answers based on query phrasing.

\textbf{\color{maincolor}The Results}

The case studies revealed striking patterns:
\begin{itemize}
\item \textbf{Text-Image Imbalance:} When asked about a garbage truck door, LLaVA gave contradictory answers ("door is open" vs. "door is closed") depending solely on query wording:relevancy maps showed 3-4x higher attention to text tokens
\item \textbf{Visual Grounding Success:} For color identification tasks with consistent queries, image relevancy dominated (>70\% of total relevance), producing accurate outputs regardless of question phrasing  
\item \textbf{Causal Structure:} CLEANN identified sparse sets of 5-15 image patches sufficient to determine specific output tokens, suggesting learned compositional reasoning
\item \textbf{Transferability:} Attention patterns generalized across different question types for the same image
\end{itemize}

\textbf{\color{maincolor}The Fine Print}

Several limitations constrain generalizability: (1) evaluation limited to a single model architecture (LLaVA), (2) frozen vision encoder prevented full gradient flow analysis, (3) MMVP dataset focuses on adversarial cases rather than typical use, (4) causal graphs derived only from final layer attention. The tool requires manual interpretation:no automated bias detection metrics provided. Computational overhead makes real-time deployment monitoring infeasible.

\textbf{\color{maincolor}The Verdict}

This work provides essential infrastructure for LVLM safety research. The discovery of systematic text-bias in visual grounding has immediate implications for medical imaging applications where textual context shouldn't override visual evidence. The tool is well-engineered and the case studies compelling. Three critical experiments would advance this work: (1) quantify text/image bias ratios across medical imaging datasets, (2) test whether architectural modifications (e.g., separate attention streams) reduce manipulation vulnerability, (3) develop automated monitoring metrics for production deployment. For healthcare AI, this tool should become part of standard validation pipelines.

\vspace{0.3em}
\hrule
\vspace{0.2em}
{\footnotesize \textbf{Key Insight:} The same multimodal flexibility that makes LVLMs powerful also creates a fundamental vulnerability:when uncertain, models default to textual priors rather than visual evidence, a particularly dangerous failure mode for medical applications.}

\begin{center}
{\Large \textbf{\color{maincolor}Token Activation Map to Visually Explain Multimodal LLMs (2025)}}\\
\vspace{0.2em}
{\small Yi Li, Hualiang Wang, Xinpeng Ding, Haonan Wang, Xiaomeng Li}
\end{center}

\vspace{-0.5em}
\begin{tcolorbox}[colback=blue!5,colframe=maincolor,boxrule=0.5pt]
\textbf{Core Discovery:}  
Multimodal LLMs generate text progressively, causing early‐token activations to pollute later explanations. Token Activation Map (TAM) disentangles each token’s true visual cues by combining an estimated causal‐inference module with a rank Gaussian filter to suppress interference and noise.
\end{tcolorbox}

\textbf{\color{maincolor}1. The Setup}\\
Multimodal LLMs (e.g., Qwen2-VL, LLaVA, InternVL) fuse vision and language for complex tasks, but explainability methods borrowed from CNNs/ViTs fail to account for inter‐token dependencies. Context tokens—including prompts and earlier outputs—introduce redundant activations that degrade the reliability of token‐level visual explanations.

\textbf{\color{maincolor}2. The Breakthrough}\\
\emph{Central Thesis:} Causally disentangling context activations and applying adaptive denoising yields faithful, high‐resolution activation maps for each generated token.  
\begin{itemize}
  \item \textbf{Estimated Causal Inference (ECI):} Models interference from all prior tokens as a weighted combination and subtracts it via least‐squares optimization to recover the pure activation for the current token.
  \item \textbf{Rank Gaussian Filter (RGF):} Applies a local rank transform weighted by a Gaussian kernel to remove salt‐and‐pepper noise while preserving fine visual details.
  \item \textbf{Multimodal Map Fusion:} Combines refined visual maps with normalized textual relevance scores for joint image–text explainability.
\end{itemize}

\textbf{\color{maincolor}3. The Blueprint}\\
\begin{itemize}
  \item \textbf{Data \& Models:} Evaluated on COCO Caption (5 000 examples), OpenPSG (3 176), GranDf (1 000), across seven MLLMs (Qwen2-VL-2B/7B, LLaVA1.5-7B/13B, InternVL2.5-2B/4B/8B).
  \item \textbf{Architecture:}  
    \begin{itemize}
      \item Extract visual features \(F^v\) and token weights \(w_t\).  
      \item Compute raw activations for each token.  
      \item Apply ECI to subtract context‐induced interference.  
      \item Denoise with RGF.  
      \item Fuse with textual relevance for final visualization.
    \end{itemize}
  \item \textbf{Evaluation Metrics:}  
    \begin{itemize}
      \item \emph{Obj-IoU}: Overlap with object masks for content words.  
      \item \emph{Func-IoU}: Overlap with background regions for function words.  
      \item \emph{F1-IoU}: Combined score measuring overall plausibility.
    \end{itemize}
\end{itemize}

\textbf{\color{maincolor}4. The Results}\\
\begin{itemize}
  \item On COCO Caption: TAM achieves 27.37 % Obj-IoU and 68.44 % Func-IoU, yielding 39.10 % F1-IoU, an 8.96‐point improvement over baseline CAM.
  \item On OpenPSG: F1-IoU increases by 8.54 points; on GranDf by 2.82 points.
  \item Consistent gains (5.45–11.00 points) across all seven tested MLLMs.
  \item Qualitative analysis shows sharper object localization, clearer attribute highlighting (color, shape, action, number, location), and better error diagnosis, including VQA misalignments and inter‐model comparisons.
\end{itemize}

\textbf{\color{maincolor}5. The Fine Print}\\
\begin{itemize}
  \item \textbf{Modal Focus:} Only visual inputs considered; audio, depth, and other modalities remain unexplored.
  \item \textbf{Plausibility vs. Faithfulness:} Metrics assess alignment with human expectations but do not guarantee causal faithfulness, as perturbation tests would disrupt text generation.
  \item \textbf{Annotation Dependence:} Requires pixel‐level masks for Obj-IoU; performance on open‐set or unannotated scenes may degrade.
  \item \textbf{Future Directions:} Extend to non‐additive causal models, integrate slice‐wise perturbation tests, and deploy TAM in real‐time or clinical pipelines.
\end{itemize}

\textbf{\color{maincolor}6. The Verdict}\\
TAM fills a key gap in MLLM interpretability by introducing a principled causal disentanglement and adaptive denoising pipeline, validated across multiple datasets and architectures. The work is methodologically sound, reproducible, and clearly presented.  
\emph{Suggested follow-up experiments:}
\begin{enumerate}
  \item Adapt TAM for audio–text and video–text models to test cross‐modal robustness.
  \item Investigate non-linear causal inference techniques (e.g., kernel methods) for complex token interactions.
  \item Evaluate the utility of TAM outputs in downstream tasks like segmentation and anomaly detection.
\end{enumerate}
Overall, the paper is well‐written, logically structured, and provides comprehensive supplementary material for replication.

\end{document}

