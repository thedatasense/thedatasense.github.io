---
layout: page
permalink: /research/
title: Research
nav: true
nav_order: 2
---

## Problem Statement

As healthcare institutions increasingly deploy Vision-Language Models (VLMs) for clinical decision support, these AI systems face critical security and reliability challenges. Subtle adversarial manipulations—imperceptible to human observers—can cause dangerous misdiagnoses or inappropriate treatment recommendations. My dissertation research addresses this fundamental barrier to safe clinical AI deployment.

---

## Research Focus

**Robust Defense Strategies for Medical Vision-Language Models**

Under the advisement of [Dr. Vahid Behzadan](https://sail-lab.org/) at the [Secure and Assured Intelligent Learning Lab (SAIL Lab)](https://unhsaillab.github.io/), University of New Haven.

My research develops comprehensive defense mechanisms for medical VLMs across three key areas:

- **Adversarial Attack Detection**: Identifying visual perturbations and malicious inputs before they compromise model outputs
- **Preprocessing Defenses**: Developing input sanitization techniques that neutralize adversarial modifications while preserving diagnostic information
- **XAI-Guided Mitigation**: Leveraging explainable AI methods to understand attack surfaces and design targeted countermeasures

---

## Key Research Questions

1. **Vulnerability Assessment**: How susceptible are state-of-the-art medical VLMs to multimodal adversarial attacks, and what unique vulnerability surfaces emerge from the integration of visual and textual modalities?

2. **Defense Mechanisms**: What preprocessing and detection strategies can effectively neutralize adversarial perturbations while maintaining diagnostic accuracy in medical imaging contexts?

3. **Clinical Safety**: How can we establish quantitative security benchmarks that healthcare institutions and regulatory bodies can use to evaluate AI systems before clinical deployment?

4. **Interpretability**: How can explainable AI techniques reveal attack patterns and guide the development of more robust medical VLM architectures?

---

## Research Thrusts

### VSF-Med: Vulnerability Scoring Framework
A comprehensive framework for evaluating security vulnerabilities in medical AI systems through text-prompt attack templates, imperceptible visual perturbations, and an eight-dimensional risk assessment rubric. Evaluated over 30,000 adversarial variants from 5,000 radiology images.

**Publication**: [arXiv:2507.00052](https://arxiv.org/abs/2507.00052)

### Fault Detection in Medical Devices
Comparative study of generative models (GAN, VAE) versus classical methods (HMM) for early detection of failures in medical devices, leveraging Data-Driven Digital Twins for predictive maintenance.

### Precision Oncology Decision Support
Integration of data-driven Physiology-Based PharmacoKinetic (PBPK) modeling with Reinforcement Learning for dynamic treatment optimization in cancer therapy.

---

## GitHub Repositories

### Primary Research
- [robust-med-mllm-experiments](https://github.com/thedatasense/robust-med-mllm-experiments) — Robustness experiments for medical multimodal LLMs
- [medical-vlm-intepret](https://github.com/thedatasense/medical-vlm-intepret) — Interpretability tools for medical VLMs

### Medical VLM Implementations
- [LLaVA-Med](https://github.com/thedatasense/LLaVA-Med) — Large Language and Vision Assistant for biomedicine
- [CheXagent](https://github.com/thedatasense/CheXagent) — Chest X-ray analysis agent
- [CARES](https://github.com/thedatasense/CARES) — Clinical AI reasoning and evaluation

### SAIL Lab Projects
- [rl_for_theranostics](https://github.com/UNHSAILLab/rl_for_theranostics) — Reinforcement learning for precision oncology
- [Fault-Detection-on-Surgical-Stapler](https://github.com/UNHSAILLab/Fault-Detection-on-Surgical-Stapler-) — Generative methods for medical device fault detection

---

## Collaborators

- **[Dr. Vahid Behzadan](https://sail-lab.org/)** — Dissertation Advisor, SAIL Lab Director
- **[SAIL Lab](https://unhsaillab.github.io/)** — Secure and Assured Intelligent Learning Lab, University of New Haven

---

## Contact

For research inquiries or collaboration opportunities, please reach out via [LinkedIn](https://www.linkedin.com/in/bineshk) or email at contact(at)bineshkumar(dot)me.
